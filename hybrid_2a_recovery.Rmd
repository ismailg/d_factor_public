---
title: "Untitled"
author: "Ismail Guennouni"
date: "`r Sys.Date()`"
output: html_document
---


```{r}
############################################################################
### 1) Libraries
############################################################################
library(tidyverse)
library(optimx)
library(lhs)
library(parallel)
library(pbapply)      # optional for parallel progress bars
library(viridis)      # for nice heatmap color scales
############################################################################


############################################################################
### 2) Basic Helper Functions
############################################################################

# Convert investor's investment into 3 discrete bins: {1=Low, 2=Med, 3=High}
get_investment_bin <- function(investment) {
  if (investment <= 7)   return(1)  # Low
  if (investment <= 14)  return(2)  # Medium
  return(3)                        # High
}

# Convert trustee's return proportion (0..1) into 6 bins
get_return_bin <- function(return_prop) {
  if (return_prop < 0 || return_prop > 1) {
    stop("Return proportion must be between 0 and 1.")
  }
  # Bin numbers: 1..6
  return(min(floor(return_prop * 6) + 1, 6))
}

# Calculate actual monetary payoffs for the trustee and investor
# Here, we assume the trustee invests 0 resources, and the investor invests (3 x investment).
# You had a version that sets "investor payoff = return - investment", but let's keep
# your definitions. Modify as needed if your real code differs.
calculate_payoffs <- function(investment, return_amount) {
  trustee_payoff  <- 3 * investment - return_amount
  investor_payoff <- return_amount - investment
  list(trustee = trustee_payoff, investor = investor_payoff)
}

# Fehr–Schmidt utility
calculate_fs_utility <- function(own_payoff, other_payoff, envy, guilt) {
  disadv_inequity <- max(other_payoff - own_payoff, 0)
  adv_inequity    <- max(own_payoff - other_payoff, 0)
  own_payoff - envy * disadv_inequity - guilt * adv_inequity
}


############################################################################
### 3) Forward Simulation Helpers (Bins -> Representative Values, Transitions)
############################################################################

# For forward-looking simulation:
bin_to_investment <- function(s_bin) {
  # Representative investment per bin.
  # You chose 4, 11, 17 in your code:
  if (s_bin == 1) return(4)   # Low
  if (s_bin == 2) return(11)  # Medium
  if (s_bin == 3) return(17)  # High
  stop("Invalid s_bin in bin_to_investment()")
}

bin_to_return_prop <- function(a_bin) {
  # Representative return proportion from bin a_bin (1..6).
  # You picked the midpoint of each bin as (2*a_bin - 1)/12
  return((2 * a_bin - 1) / 12)
}

# Immediate FS payoff for a given state-action pair
fs_payoff_state_action <- function(s_bin, a_bin, envy, guilt) {
  inv  <- bin_to_investment(s_bin)
  prop <- bin_to_return_prop(a_bin)
  ret  <- round(prop * (3 * inv))  # round if you want integer returns
  
  trustee_payoff  <- 3 * inv - ret
  investor_payoff <- ret - inv
  
  disadv_inequity <- max(investor_payoff - trustee_payoff, 0)
  adv_inequity    <- max(trustee_payoff - investor_payoff, 0)
  u <- trustee_payoff - envy * disadv_inequity - guilt * adv_inequity
  return(u)
}

# Transition probabilities for next state given current state-action
transition_probs_state_action <- function(s_bin, a_bin, sensitivity) {
  inv  <- bin_to_investment(s_bin)
  prop <- bin_to_return_prop(a_bin)
  ret  <- prop * (3 * inv)
  net_return <- ret - inv
  
  probs <- rep(0, 3)
  if (s_bin == 1) {
    p_up      <- exp(sensitivity * net_return)
    p_stay    <- 1
    norm_const <- p_up + p_stay
    probs[1] <- 0                  
    probs[2] <- p_stay / norm_const
    probs[3] <- p_up   / norm_const
  } else if (s_bin == 3) {
    p_down     <- exp(-sensitivity * net_return)
    p_stay     <- 1
    norm_const <- p_down + p_stay
    probs[1]   <- p_down / norm_const
    probs[2]   <- p_stay / norm_const
    probs[3]   <- 0                
  } else {  # s_bin == 2
    p_up   <- exp(sensitivity * net_return)
    p_down <- exp(-sensitivity * net_return)
    p_stay <- 1
    norm_const <- p_up + p_down + p_stay
    probs[1] <- p_down / norm_const
    probs[2] <- p_stay / norm_const
    probs[3] <- p_up   / norm_const
  }
  return(probs)
}


############################################################################
### 4) k-Step Lookahead via Dynamic Programming (DP)
############################################################################

compute_dp_value <- function(Q_MB_current, envy, guilt, sensitivity, plan_depth) {
  # 1) Precompute immediate rewards R(s,a) and transition probabilities P(s,a,*)
  R <- matrix(0, nrow = 3, ncol = 6)
  P_array <- array(0, dim = c(3, 6, 3))
  for (s in 1:3) {
    for (a in 1:6) {
      R[s, a] <- fs_payoff_state_action(s, a, envy, guilt)
      P_array[s, a, ] <- transition_probs_state_action(s, a, sensitivity)
    }
  }
  
  # 2) Base case: planning depth = 1
  #    dp[s,a] = R[s,a] + sum_{s'} P(s,a,s') * max_{a'} Q_MB_current[s', a']
  max_Q <- apply(Q_MB_current, 1, max)
  dp <- matrix(0, nrow = 3, ncol = 6)
  for (s in 1:3) {
    for (a in 1:6) {
      dp[s, a] <- R[s, a] + sum(P_array[s, a, ] * max_Q)
    }
  }
  
  # 3) For depths 2..plan_depth, do iterative backups
  if (plan_depth >= 2) {
    for (d in 2:plan_depth) {
      max_dp <- apply(dp, 1, max)
      for (s in 1:3) {
        for (a in 1:6) {
          dp[s, a] <- R[s, a] + sum(P_array[s, a, ] * max_dp)
        }
      }
    }
  }
  return(dp)
}


############################################################################
### 5) Priors (Optional)
############################################################################
get_literature_priors <- function() {
  list(
    means = c(
      envy = 2.0, guilt = 0.5, 
      temp = 5.0, 
      sensitivity = 0.5,
      alpha_MF = 0.3, 
      alpha_MB = 0.3, 
      w = 0.5
    ),
    sds = c(
      envy = 1.0, guilt = 0.3,
      temp = 2.0, 
      sensitivity = 0.3, 
      alpha_MF = 0.2, 
      alpha_MB = 0.2, 
      w = 0.2
    )
  )
}


############################################################################
### 6) Hybrid Model with K-step MB (Now Storing Trial-by-Trial Info)
############################################################################

trustee_decision_hybrid_k_details <- function(params, data, plan_depth,
                                              use_priors = FALSE,
                                              store_qs   = FALSE) {
  # Expect 7 parameters: envy, guilt, temp, sens, alpha_MF, alpha_MB, w
  if (length(params) != 7) {
    stop("Requires 7 parameters: envy, guilt, temp, sensitivity, alpha_MF, alpha_MB, w")
  }
  
  envy        <- params[1]
  guilt       <- params[2]
  temp        <- params[3]
  sensitivity <- params[4]
  alpha_MF    <- params[5]
  alpha_MB    <- params[6]
  w           <- params[7]
  
  # Basic checks
  if (any(is.na(params)) || any(params < 0) || w > 1) {
    return(list(nll=1e10, trial_lik=rep(NA, nrow(data)), Q_MF_list=NULL, Q_MB_list=NULL))
  }
  
  n_trials    <- nrow(data)
  log_lik     <- 0
  trial_log   <- rep(NA, n_trials)  # store trial-by-trial log-likelihood
  
  # Q tables
  Q_MF <- matrix(0, nrow=3, ncol=6)
  Q_MB <- matrix(0, nrow=3, ncol=6)
  
  # optionally store a list of Q states at each trial
  Q_MF_history <- list()
  Q_MB_history <- list()
  
  current_game <- NULL
  
  for (t in seq_len(n_trials)) {
    investment  <- data$investment[t]
    return_amt  <- data$return[t]
    game_label  <- data$gameNum.f[t]
    if (is.na(investment) || investment <= 0) next
    
    if (investment <= 0) {
      # skip invalid
      next
    }
    return_prop <- return_amt / (3 * investment)
    if (return_prop < 0 || return_prop > 1) {
      next
    }
    
    # Reset Q-values if game changes
    if (!is.null(current_game) && game_label != current_game) {
      Q_MF <- matrix(0, 3, 6)
      Q_MB <- matrix(0, 3, 6)
    }
    current_game <- game_label
    
    # State/Action
    s_bin <- get_investment_bin(investment)
    a_bin <- get_return_bin(return_prop)
    
    # Fehr–Schmidt reward
    payoffs <- calculate_payoffs(investment, return_amt)
    reward  <- calculate_fs_utility(payoffs$trustee, payoffs$investor, envy, guilt)
    
    # 1) Model-Free update
    if (t < n_trials) {
      inv_next <- data$investment[t+1]
      if (inv_next > 0) {
        s_next <- get_investment_bin(inv_next)
        td_err_MF <- reward + max(Q_MF[s_next, ]) - Q_MF[s_bin, a_bin]
        Q_MF[s_bin, a_bin] <- Q_MF[s_bin, a_bin] + alpha_MF * td_err_MF
      }
    }
    
    # 2) Model-Based update
    dp_values <- compute_dp_value(Q_MB, envy, guilt, sensitivity, plan_depth)
    val_k     <- dp_values[s_bin, a_bin]
    td_err_MB <- val_k - Q_MB[s_bin, a_bin]
    Q_MB[s_bin, a_bin] <- Q_MB[s_bin, a_bin] + alpha_MB * td_err_MB
    
    # 3) Hybrid action selection
    Q_hybrid_state <- w * Q_MB[s_bin, ] + (1 - w) * Q_MF[s_bin, ]
    centered <- Q_hybrid_state - max(Q_hybrid_state)
    probs    <- exp(centered / temp)
    probs    <- probs / sum(probs)
    probs    <- pmax(probs, 1e-10)
    
    # accumulate log-likelihood
    log_lik      <- log_lik + log(probs[a_bin])
    trial_log[t] <- log(probs[a_bin])
    
    # store Qs if desired
    if (store_qs) {
      Q_MF_history[[t]] <- Q_MF
      Q_MB_history[[t]] <- Q_MB
    }
  }
  
  if (use_priors) {
    priors <- get_literature_priors()
    prior_terms <- sum(dnorm(params, mean = priors$means, sd = priors$sds, log = TRUE))
    log_lik <- log_lik + prior_terms
  }
  
  return(list(
    nll         = -log_lik, 
    trial_lik   = trial_log,
    Q_MF_list   = if (store_qs) Q_MF_history else NULL,
    Q_MB_list   = if (store_qs) Q_MB_history else NULL
  ))
}


############################################################################
# 1) Objective Function (Unchanged)
############################################################################
trustee_decision_hybrid_k_obj <- function(par, data, plan_depth, use_priors) {
  res <- trustee_decision_hybrid_k_details(par, data, plan_depth, use_priors, store_qs=FALSE)
  return(res$nll)
}

############################################################################
# 2) Main Fitting Function with LRT
############################################################################
fit_participant_hybrid_k <- function(participant_data,
                                     max_k = 3,
                                     hierarchical = FALSE,
                                     n_multistart = 5,
                                     alpha_level = 0.05) {
  # Parameter bounds for 7 parameters
  lower <- c(
    envy = 0.01, guilt = 0.01, 
    temp = 0.1,
    sensitivity = 0.01,
    alpha_MF = 0.01, 
    alpha_MB = 0.01, 
    w = 0
  )
  upper <- c(
    envy = 6, guilt = 2,
    temp = 15, 
    sensitivity = 2, 
    alpha_MF = 0.99, 
    alpha_MB = 0.99, 
    w = 1
  )
  
  # We'll store the best result for each k
  best_by_k <- vector("list", max_k)
  
  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  # Run multi-start search for each k in [1..max_k], store best solution.
  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  for (k_test in seq_len(max_k)) {
    lhs_samples <- randomLHS(n_multistart, length(lower))
    init_params_list <- lapply(seq_len(n_multistart), function(i) {
      lower + (upper - lower) * lhs_samples[i, ]
    })
    
    fits_k <- lapply(init_params_list, function(start_par) {
      tryCatch({
        out <- optimx(
          par     = start_par,
          fn      = function(x) trustee_decision_hybrid_k_obj(
            x, participant_data, 
            plan_depth = k_test, 
            use_priors = hierarchical
          ),
          method  = "L-BFGS-B",
          lower   = lower,
          upper   = upper,
          control = list(
            maxit    = 1000,
            parscale = upper - lower,
            dowarn   = FALSE
          )
        )
        out
      }, error = function(e) NULL)
    })
    
    # Remove any failed fits
    fits_k <- fits_k[!sapply(fits_k, is.null)]
    if (length(fits_k) == 0) {
      next  # no successful fits at this k
    }
    
    # Find best solution among these fits
    obj_values <- sapply(fits_k, function(x) x$value[1])
    best_idx   <- which.min(obj_values)
    best_fit_k <- fits_k[[best_idx]]
    
    # Extract best param set
    if (!is.null(best_fit_k)) {
      best_nll_k    <- best_fit_k$value[1]
      # first 7 columns of best_fit_k are the parameters
      best_params_k <- setNames(as.numeric(best_fit_k[1,1:7]), names(lower))
      
      # Store
      best_by_k[[k_test]] <- list(
        nll       = best_nll_k,
        parameters= best_params_k
      )
    }
  }
  
  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  # If ALL fail, return an error
  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  all_null <- all(sapply(best_by_k, is.null))
  if (all_null) {
    return(list(error = "All optimizations failed for all k"))
  }
  
  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  # 3) Likelihood-Ratio Test to pick final k
  #    We'll do a simple incremental approach:
  #    - Start with the first valid k
  #    - For each new k, do an LRT. If significantly better => upgrade
  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  
  # We'll define a small helper function for the LRT.
  # Approx: difference in nll is ~ chisq with df=1 for an integer step in k.
  is_significantly_better <- function(nll_small, nll_big, alpha = alpha_level) {
    # LRT statistic = 2*(nll_big - nll_small), compare to chisq_1
    test_stat <- 2 * (nll_big - nll_small)
    crit_val  <- qchisq(1 - alpha, df = 1)  # ~3.84 for alpha=0.05
    test_stat > crit_val
  }
  
  chosen_k <- NA
  chosen_nll <- Inf
  chosen_par <- NULL
  
  for (k_test in seq_len(max_k)) {
    if (is.null(best_by_k[[k_test]])) {
      # no success at this k
      next
    }
    if (is.na(chosen_k)) {
      # first valid k we find
      chosen_k  <- k_test
      chosen_nll<- best_by_k[[k_test]]$nll
      chosen_par<- best_by_k[[k_test]]$parameters
    } else {
      # compare to current best
      cand_nll <- best_by_k[[k_test]]$nll
      if (cand_nll < chosen_nll) {
        # check LRT significance
        if (is_significantly_better(cand_nll, chosen_nll, alpha_level)) {
          chosen_k   <- k_test
          chosen_nll <- cand_nll
          chosen_par <- best_by_k[[k_test]]$parameters
        }
        # else we keep the old model
      }
    }
  }
  
  # If chosen_k still NA => all fits failed (should be impossible now, but just in case)
  if (is.na(chosen_k)) {
    return(list(error = "All optimizations failed for all k"))
  }
  
  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  # 4) Prepare final return structure (same as original)
  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  nll_final <- chosen_nll
  par_final <- chosen_par
  n_params  <- length(par_final) # ignoring k as a param, or treat it as +1 param if you prefer
  n_obs     <- sum(participant_data$investment > 0)
  
  aic_val <- 2 * nll_final + 2 * n_params
  bic_val <- 2 * nll_final + log(n_obs) * n_params
  
  return(list(
    nll        = nll_final,
    k          = chosen_k,
    parameters = par_final,
    aic        = aic_val,
    bic        = bic_val
  ))
}


```

```{r}

############################################################################
### 8) Example: Extract Trial-by-Trial Likelihood & Q's after Best Fit
############################################################################

# We'll create a small utility that, given best-fit parameters & plan_depth, 
# re-runs trustee_decision_hybrid_k_details() with store_qs=TRUE.

extract_trial_details <- function(params, data, plan_depth, use_priors=FALSE) {
  # store_qs=TRUE means we get Q_MF_list and Q_MB_list
  res <- trustee_decision_hybrid_k_details(
    params, data, plan_depth, use_priors, store_qs=TRUE
  )
  return(res)
}


############################################################################
### 9) Plotting a Heat Map of Q-values
############################################################################

# We'll define a helper that transforms a Q-table into a data frame
q_matrix_to_df <- function(Q_mat, trial_idx, q_type="MF") {
  # Q_mat is 3x6
  expand.grid(state=1:3, action=1:6) %>%
    mutate(
      Q_value = mapply(function(s,a) Q_mat[s,a], state, action),
      trial = trial_idx,
      Q_type = q_type
    )
}

plot_heatmap_q <- function(Q_history, trials_to_show=NULL, Q_type="MF") {
  # Q_history is a list of Q matrices, one per trial
  # trials_to_show: vector of trial indices you want to plot
  if (is.null(trials_to_show)) {
    # e.g. pick some regularly spaced or last few?
    trials_to_show <- seq(1, length(Q_history), length.out=4)
  }
  # build a df
  df_q <- do.call(rbind, lapply(trials_to_show, function(tt) {
    if (tt <= length(Q_history) && !is.null(Q_history[[tt]])) {
      q_matrix_to_df(Q_history[[tt]], trial_idx=tt, q_type=Q_type)
    } else {
      NULL
    }
  }))
  # plot
  ggplot(df_q, aes(x=action, y=factor(state), fill=Q_value)) +
    geom_tile() +
    facet_wrap(~ trial) +
    scale_fill_viridis() +
    theme_minimal() +
    labs(title=paste("Q", Q_type, "Heat Map"), x="Action", y="State")
}




```


```{r}
############################################################################
### Usage Examples
############################################################################
#    We fit the model:
fit_res <- fit_participant_hybrid_k(test_data1, max_k=2, hierarchical=TRUE)
best_params <- fit_res$parameters
best_k      <- fit_res$k

# 2) We get the trial-by-trial details:
details <- extract_trial_details(best_params, test_data1, best_k, use_priors=TRUE)

  # - details$trial_lik => vector of log-likelihood per trial
  # - details$Q_MF_list, details$Q_MB_list => lists of Q mat at each trial

# 3) Plot the Q-heatmap for a few selected trials:
plot_heatmap_q(details$Q_MF_list, trials_to_show=c(1, 7, 15, 24), Q_type="MF")
plot_heatmap_q(details$Q_MB_list, trials_to_show=c(1, 7, 15, 24), Q_type="MB")

```
# parameter recovery 

```{r}
updateState <- function(state, PnL) {
  if (state == "unhappy") {
    mod <- c(
      exp(0.0 + 0.0 * PnL),
      exp(-3.366027  + 0.40910797 * PnL),
      exp(-3.572619  - 0.08137274 * PnL)
    )
  } else if (state == "neutral") {
    mod <- c(
      exp(0.0 + 0.0 * PnL),
      exp(3.3142637  + 0.3763408  * PnL),
      exp(0.9169736  + 0.4502838  * PnL)
    )
  } else if (state == "happy") {
    mod <- c(
      exp(0.0 + 0.0 * PnL),
      exp(0.7134085  + 0.02101626 * PnL),
      exp(2.2215478  + 0.16162964 * PnL)
    )
  } else {
    stop("updateState: unknown current state.")
  }
  mod <- mod / sum(mod)
  new_states <- c("unhappy","neutral","happy")
  return(new_states[sampleCat(mod)])
}

updateState_vol <- function(state, PnL) {
  if (state == "unhappy") {
    mod <- c(
      exp(0.0 + 0.0 * PnL),
      exp(-3.366027  + 0.40910797 * PnL),
      exp(-3.572619  - 0.08137274 * PnL)
    )
  } else if (state == "neutral") {
    mod <- c(
      exp(0.0 + 0.0 * PnL),
      exp(1 + 0.27 * PnL),
      exp(-4 + 0.75 * PnL)
    )
  } else if (state == "happy") {
    mod <- c(
      exp(0.0 + 0.0 * PnL),
      exp(0.7134085  + 0.02101626 * PnL),
      exp(2.2215478  + 0.16162964 * PnL)
    )
  } else {
    stop("updateState_vol: unknown current state.")
  }
  mod <- mod / sum(mod)
  new_states <- c("unhappy","neutral","happy")
  return(new_states[sampleCat(mod)])
}

# Sample from a categorical distribution given probability vector 'prob'
sampleCat <- function(prob) {
  sample(seq_along(prob), size = 1, prob = prob)
}
```


```{r}

simulate_2games_data_k <- function(params, true_k, game_size = 25, playerId = 1) {
  # params: vector of 7 continuous parameters:
  #   [1] envy, [2] guilt, [3] temp, [4] sensitivity,
  #   [5] alpha_MF, [6] alpha_MB, [7] w
  if (length(params) != 7) {
    stop("simulate_2games_data_k_refactored expects 7 parameters: envy, guilt, temp, sensitivity, alpha_MF, alpha_MB, w")
  }
  
  # Unpack parameters
  envy        <- params[1]
  guilt       <- params[2]
  temp        <- params[3]
  sensitivity <- params[4]
  alpha_MF    <- params[5]
  alpha_MB    <- params[6]
  w           <- params[7]
  
  total_rounds <- 2 * game_size
  df <- data.frame(
    trial      = 1:total_rounds,
    investment = rep(NA, total_rounds),
    return     = rep(NA, total_rounds),
    gameNum.f  = factor(rep("first game", total_rounds),
                        levels = c("first game", "second game"))
  )
  df$gameNum.f[(game_size + 1):total_rounds] <- "second game"
  
  simulate_one_game <- function(start_idx, end_idx, game_label) {
    # Reset Q-values for both model–free (MF) and model–based (MB) systems.
    Q_MF <- matrix(0, nrow = 3, ncol = 6)
    Q_MB <- matrix(0, nrow = 3, ncol = 6)
    
    # For simulating investor actions, we assume the true process governs state transitions.
    # We keep the investor's state as a string.
    current_state <- "neutral"
    
    for (t in seq(start_idx, end_idx)) {
      # Convert the current state (string) to a bin (integer index)
      s_bin <- state_to_bin(current_state)
      
      # --- Hybrid Action Selection ---
      Q_hybrid <- w * Q_MB[s_bin, ] + (1 - w) * Q_MF[s_bin, ]
      centered <- Q_hybrid - max(Q_hybrid)
      exp_vals <- exp(centered / temp)
      if (!all(is.finite(exp_vals)) || sum(exp_vals) == 0) {
        probs <- rep(1/6, 6)
      } else {
        probs <- exp_vals / sum(exp_vals)
      }
      a_bin <- sample(1:6, 1, prob = probs)
      
      # --- Compute Investment and Return ---
      investment <- bin_to_investment(s_bin)
      ret_prop   <- bin_to_return_prop(a_bin)
      return_amt <- round(ret_prop * (3 * investment))
      
      df$investment[t] <<- investment
      df$return[t]     <<- return_amt
      
      # --- Compute Fehr–Schmidt Reward ---
      payoffs <- calculate_payoffs(investment, return_amt)
      rew     <- calculate_fs_utility(payoffs$trustee, payoffs$investor, envy, guilt)
      
      # --- MODEL-FREE Update and Investor State Transition ---
      # Here we assume the investor's actual state transitions follow the ground truth.
      if (t < end_idx) {
        PnL <- return_amt - investment
        if (game_label == "first game") {
          next_state <- updateState(current_state, PnL)
        } else {
          next_state <- updateState_vol(current_state, PnL)
        }
        next_s_bin <- state_to_bin(next_state)
        td_err_MF <- rew + max(Q_MF[next_s_bin, ]) - Q_MF[s_bin, a_bin]
        Q_MF[s_bin, a_bin] <- Q_MF[s_bin, a_bin] + alpha_MF * td_err_MF
        current_state <- next_state  # update true investor state
      }
      
      # --- MODEL-BASED (Planning) Update ---
      # Planning uses the participant's learned model of the environment,
      # which is sensitive to the sensitivity parameter.
      dp_vals <- compute_dp_value(Q_MB, envy, guilt, sensitivity, true_k)
      val_k   <- dp_vals[s_bin, a_bin]
      td_err_MB <- val_k - Q_MB[s_bin, a_bin]
      Q_MB[s_bin, a_bin] <- Q_MB[s_bin, a_bin] + alpha_MB * td_err_MB
    }
  }
  
  # Simulate the two games (each with its own state transitions).
  simulate_one_game(1, game_size, "first game")
  simulate_one_game(game_size + 1, total_rounds, "second game")
  
  df$playerId <- playerId
  return(df)
}



# simulate_2games_data_k <- function(params, true_k, game_size=25) {
#   # params: vector of 7 continuous parameters (envy, guilt, temp, sensitivity, alpha_MF, alpha_MB, w)
#   # true_k: integer in {1..max_k}
#   # game_size: number of rounds per game (25 in your real experiment)
#   #
#   # We'll produce a data.frame of 2 * game_size rows, 
#   # with a 'gameNum.f' column taking values "first game" or "second game".
#   #
#   # Q-values are reset to 0 at the start of each game, 
#   # as in your real code.
#   if (length(params) != 7) {
#     stop("simulate_2games_data_k expects 7 continuous params")
#   }
#   
#   # Unpack
#   envy        <- params[1]
#   guilt       <- params[2]
#   temp        <- params[3]
#   sensitivity <- params[4]
#   alpha_MF    <- params[5]
#   alpha_MB    <- params[6]
#   w           <- params[7]
#   
#   total_rounds <- 2 * game_size
#   df <- data.frame(
#     trial      = 1:total_rounds,
#     investment = rep(NA, total_rounds),
#     return     = rep(NA, total_rounds),
#     gameNum.f  = factor(rep("first game", total_rounds), 
#                         levels=c("first game","second game"))
#   )
#   df$gameNum.f[(game_size + 1):total_rounds] <- "second game"
#   
#   simulate_one_game <- function(start_idx, end_idx, game_label) {
#     # Q-values reset
#     Q_MF <- matrix(0,3,6)
#     Q_MB <- matrix(0,3,6)
#     # Investor state starts in medium
#     s_bin <- 2
#     
#     for (t in seq(start_idx, end_idx)) {
#       # HYBRID action selection
#       Q_hybrid <- w*Q_MB[s_bin,] + (1-w)*Q_MF[s_bin,]
#       centered <- Q_hybrid - max(Q_hybrid)
#       exp_vals <- exp(centered / temp)
#       
#       # If sum(exp_vals) is Inf or 0, we do a fallback:
#       if (!all(is.finite(exp_vals)) || sum(exp_vals)==0) {
#         # handle gracefully
#         # e.g. pick uniform random action
#         probs <- rep(1/6, 6)
#       } else {
#         probs <- exp_vals / sum(exp_vals)
#       }
#       
#       a_bin <- sample(1:6, 1, prob=probs)
#       
#       # compute investment, return
#       investment <- bin_to_investment(s_bin)
#       if (is.na(investment)) {
#         # skip or break or fix
#         next
#       }
#       ret_prop   <- bin_to_return_prop(a_bin)
#       return_amt <- round(ret_prop * (3*investment))
#       
#       df$investment[t] <- investment
#       df$return[t]     <- return_amt
#       
#       # Fehr–Schmidt reward
#       payoffs <- calculate_payoffs(investment, return_amt)
#       rew     <- calculate_fs_utility(payoffs$trustee, payoffs$investor, envy, guilt)
#       
#       # MODEL-FREE update (only if not last trial of the game)
#       if (t < end_idx) {
#         # compute transition
#         trans <- transition_probs_state_action(s_bin, a_bin, sensitivity)
#         # check if valid
#         if (any(!is.finite(trans)) || sum(trans)<1e-12) {
#           # fallback if invalid
#           trans <- c(1/3, 1/3, 1/3)
#         }
#         s_next <- sample(1:3,1,prob=trans)
#         
#         td_err_MF <- rew + max(Q_MF[s_next,]) - Q_MF[s_bin,a_bin]
#         Q_MF[s_bin,a_bin] <- Q_MF[s_bin,a_bin] + alpha_MF * td_err_MF
#         
#         s_bin <- s_next
#       }
#       
#       # MODEL-BASED update with `true_k`
#       dp_vals <- compute_dp_value(Q_MB, envy, guilt, sensitivity, true_k)
#       val_k   <- dp_vals[s_bin,a_bin]
#       td_err_MB <- val_k - Q_MB[s_bin,a_bin]
#       Q_MB[s_bin,a_bin] <- Q_MB[s_bin,a_bin] + alpha_MB * td_err_MB
#     }
#   }
#   
#   # First game: 1..game_size
#   simulate_one_game(1, game_size, "first game")
#   # Second game: game_size+1..2*game_size
#   simulate_one_game(game_size+1, total_rounds, "second game")
#   
#   return(df)
# }

```


```{r}
param_recovery_2games_experiment <- function(n_sims=100, max_k=3, game_size=25) {
  # total trials = 2 * game_size = 50 typically
  # For each "participant," we:
  #  1) randomly sample param set
  #  2) pick a random true_k
  #  3) simulate data with 2 games
  #  4) fit over k=1..max_k
  #  5) store results
  
  #set.seed(999)
  results <- data.frame()
  
  for (i in seq_len(n_sims)) {
    # sample random continuous parameters
    envy        <- runif(1, 0.01, 2)
    guilt       <- runif(1, 0.01, 1)
    temp        <- runif(1, 0.1, 5)
    sensitivity <- runif(1, 0.01, 2)
    alpha_MF    <- runif(1, 0.01, 0.99)
    alpha_MB    <- runif(1, 0.01, 0.99)
    w           <- runif(1, 0, 1)
    
    params_true <- c(envy, guilt, temp, sensitivity, alpha_MF, alpha_MB, w)
    true_k      <- sample(1:max_k, 1)
    
    # cat(sprintf("Participant %d: envy=%.3f, guilt=%.3f, temp=%.3f,  sensitivity=%.3f, 
    #             alpha_MF=%.3f,    alpha_MF=%.3f   w=%.3f, k=%.0f\n",
    #             i, envy, guilt,temp, sensitivity, alpha_MF, alpha_MB,  w, true_k))
    
    # simulate 2*game_size trials splitted in 2 games
    sim_data <- simulate_2games_data_k(params_true, true_k, game_size)
    
    # fit the participant
    fit_res <- fit_participant_hybrid_k(sim_data, max_k=max_k, n_multistart=2)
    
    if (!is.null(fit_res$parameters)) {
      fitted_par <- fit_res$parameters
      df_one <- data.frame(
        sim_id   = i,
        true_envy= envy,
        fit_envy = fitted_par["envy"],
        true_guilt= guilt,
        fit_guilt = fitted_par["guilt"],
        true_temp = temp,
        fit_temp = fitted_par["temp"],
        true_sensitivity = sensitivity, 
        fit_sensitivity = fitted_par["sensitivity"],
        true_alpha_MF = alpha_MF,
        fit_alpha_MF = fitted_par["alpha_MF"],
        true_alpha_MB = alpha_MB,
        fit_alpha_MB = fitted_par["alpha_MB"],
        true_w   = w,
        fit_w    = fitted_par["w"],
        true_k   = true_k,
        fit_k    = fit_res$k,
        nll      = fit_res$nll
      )
      results <- rbind(results, df_one)
    } else {
      df_fail <- data.frame(
        sim_id = i,
        true_k = true_k,
        fit_k  = NA,
        error  = "Fit failed"
      )
      results <- rbind(results, df_fail)
    }
  }
  
  return(results)
}

```

```{r}
analyze_recovery_2games <- function(results_df) {
  good_df <- results_df %>% filter(!is.na(fit_k))
  
  cat("Confusion matrix for k:\n")
  print(table(good_df$true_k, good_df$fit_k))
  
param_pairs <- list(
  c("true_envy",        "fit_envy"),
  c("true_guilt",       "fit_guilt"),
  c("true_temp",        "fit_temp"),
  c("true_sensitivity", "fit_sensitivity"),
  c("true_alpha_MF",    "fit_alpha_MF"),
  c("true_alpha_MB",    "fit_alpha_MB"),
  c("true_w",           "fit_w")
)
  
  for (pp in param_pairs) {
    pt  <- pp[1]
    pf  <- pp[2]
    cor_val <- cor(good_df[[pt]], good_df[[pf]], use="complete.obs")
    diffs   <- good_df[[pt]] - good_df[[pf]]
    mse_val <- mean(diffs^2, na.rm=TRUE)
    cat(sprintf("%s vs %s => r=%.3f, MSE=%.3f\n", pt, pf, cor_val, mse_val))
  }
}

```

```{r}
# 1) Run the simulation-based experiment
recover_res <- param_recovery_2games_experiment(
  n_sims = 100,    # 100 participants
  max_k  = 3, 
  game_size = 25   # so total 50 trials per "participant"
)

recover_res 

# 2) Analyze 
analyze_recovery_2games(recover_res)

```
```{r}
library(dplyr)

# List the param "base names" you want
my_params <- c("envy", "guilt", "alpha_MB", "alpha_MF", "sensitivity", "w")

# Build a "long" data frame with columns: param, true_val, fit_val
df_plot <- do.call(rbind, lapply(my_params, function(par_name) {
  data.frame(
    param    = par_name,
    true_val = recover_res[[paste0("true_", par_name)]],
    fit_val  = recover_res[[paste0("fit_",  par_name)]]
  )
}))

df_cor <- df_plot %>%
  group_by(param) %>%
  summarise(
    r = cor(true_val, fit_val, use="complete.obs")  # correlation
  )

df_plot <- df_plot %>%
  left_join(df_cor, by="param")

library(ggplot2)

ggplot(df_plot, aes(x=true_val, y=fit_val)) +
  geom_point(alpha = 0.5) +
  facet_wrap(~ param, scales = "free") +
  geom_smooth(method="lm", color="red", se=FALSE) +
  # Optional: place correlation text in a corner
  geom_text(
    data = df_cor,  # we only need one label per facet
    aes(
      x = Inf, 
      y = -Inf,
      label = paste0("r = ", round(r, 2))
    ),
    hjust = 1.1,    # shift slightly inside the plot
    vjust = -0.5,
    color = "blue"
  ) +
  theme_minimal() +
  labs(
    title    = "True vs. Fitted Parameters",
    x        = "True value",
    y        = "Fitted value"
  )
```






