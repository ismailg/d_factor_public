---
title: "new_MFRL"
author: "Ismail Guennouni"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Fit MFRL to data

## Helper functions
```{r}
###############################################################################
# 1) Libraries
###############################################################################
library(tidyverse)
library(optimx)
library(lhs)
library(parallel)
library(pbapply)

###############################################################################
# 2) Basic Helper Functions
###############################################################################

get_investment_bin <- function(investment) {
  if (investment <= 7)   return(1)
  if (investment <= 14)  return(2)
  return(3)
}

get_return_bin <- function(return_prop) {
  if (return_prop < 0 || return_prop > 1) 
    stop("Return proportion must be between 0 and 1.")
  # Divide [0,1] into 6 equal bins.
  return(min(floor(return_prop * 6) + 1, 6))
}

# Fehr–Schmidt utility function (as used in the hybrid model)
calculate_fs_utility <- function(own_payoff, other_payoff, envy, guilt) {
  disadv <- max(other_payoff - own_payoff, 0)
  adv <- max(own_payoff - other_payoff, 0)
  own_payoff - envy * disadv - guilt * adv
}


```

```{r}

# State => {1,2,3} => {unhappy, neutral, happy}
state_to_bin <- function(state) {
  if (state == "unhappy") return(1)
  if (state == "neutral") return(2)
  if (state == "happy")   return(3)
  stop("state_to_bin: unknown state")
}

# Bins => Investments
#   1 => 4   2 => 11   3 => 17
bin_to_investment <- function(s_bin) {
  if (s_bin == 1) return(4)
  if (s_bin == 2) return(11)
  if (s_bin == 3) return(17)
  stop("Invalid s_bin in bin_to_investment()")
}

# Return proportion from a_bin ∈ {1..6} => midpoints of 6 equal bins in [0,1]
# a_bin => (2*a_bin - 1)/12
bin_to_return_prop <- function(a_bin) {
  return((2 * a_bin - 1) / 12)
}

```




# Fitting functions

```{r}
###############################################################################
# 3) Model Function: direct_learning_model2
###############################################################################
# This function implements a Q–learning model with different learning rates and
# softmax temperatures for game 1 and game 2, and uses a Fehr–Schmidt utility
# function to compute rewards.
#
# Parameter vector (length = 6):
#   params[1] = alpha_game1
#   params[2] = alpha_game2
#   params[3] = temp_game1
#   params[4] = temp_game2
#   params[5] = envy
#   params[6] = guilt
#
# The discount factor is fixed at 1. Q–values are re–initialized at the start
# of each game.
direct_learning_model2 <- function(params, data) {
  if (length(params) != 6) 
    stop("direct_learning_model2 requires 6 parameters: alpha_game1, alpha_game2, temp_game1, temp_game2, envy, guilt")
  
  alpha_game1 <- params[1]
  alpha_game2 <- params[2]
  temp_game1  <- params[3]
  temp_game2  <- params[4]
  envy        <- params[5]
  guilt       <- params[6]
  gamma       <- 1  # Fixed
  
  n_trials <- nrow(data)
  log_lik <- 0
  n_actions <- 6
  Q_values <- matrix(0, nrow = 3, ncol = n_actions)
  
  current_game <- data$gameNum.f[1]
  
  for (t in seq_len(n_trials)) {
    investment <- data$investment[t]
    actual_return <- data$return[t]
    
    if (is.na(investment) || is.na(actual_return) || investment == 0) next
    
    # Reset Q-values at the start of a new game.
    if (t > 1 && data$gameNum.f[t] != current_game) {
      Q_values <- matrix(0, nrow = 3, ncol = n_actions)
      current_game <- data$gameNum.f[t]
    }
    
    # Choose parameters based on the game.
    if (current_game == "first game") {
      alpha <- alpha_game1
      temp  <- temp_game1
    } else {
      alpha <- alpha_game2
      temp  <- temp_game2
    }
    
    state <- get_investment_bin(investment)
    current_Q <- Q_values[state, ]
    current_Q_shift <- current_Q - max(current_Q)
    probs <- exp(current_Q_shift / temp)
    probs <- probs / sum(probs)
    probs <- pmax(probs, 1e-10)
    probs <- probs / sum(probs)
    
    actual_prop <- actual_return / (3 * investment)
    chosen_action <- get_return_bin(actual_prop)
    
    log_lik <- log_lik + log(probs[chosen_action])
    
    # Compute payoffs.
    trustee_payoff <- 3 * investment - actual_return
    investor_payoff <- actual_return - investment
    reward <- calculate_fs_utility(trustee_payoff, investor_payoff, envy, guilt)
    
    # Look ahead: get future value from next state if available.
    if (t < n_trials) {
      next_invest <- data$investment[t + 1]
      if (!is.na(next_invest) && next_invest > 0) {
        next_state <- get_investment_bin(next_invest)
        future_val <- max(Q_values[next_state, ])
      } else {
        future_val <- 0
      }
    } else {
      future_val <- 0
    }
    
    pe <- reward + gamma * future_val - Q_values[state, chosen_action]
    Q_values[state, chosen_action] <- Q_values[state, chosen_action] + alpha * pe
  }
  
  return(-log_lik)
}

###############################################################################
# 4A) Full–Data Fitting Functions
###############################################################################
# (a) Fit a single participant using all rounds.
fit_direct_model2_single <- function(participant_data, n_multistart = 20) {
  # Create a round index if not present.
  if (!"roundNum" %in% names(participant_data)) {
    participant_data <- participant_data %>%
      group_by(gameNum.f, playerId) %>% 
      mutate(roundNum = row_number()) %>% 
      ungroup()
  }
  
  # Parameter bounds:
  # alpha_game1, alpha_game2 in (0.001, 1)
  # temp_game1, temp_game2 in (0.1, 15)
  # envy in (0.01, 5), guilt (0.01, 2)
  lower <- c(0.001, 0.001, 0.1, 0.1, 0.01, 0.01)
  upper <- c(1, 1, 15, 15, 5, 2)
  
  # set.seed(123)
  best_nll <- Inf
  best_params <- NULL
  
  lhs_samples <- randomLHS(n_multistart, length(lower))
  init_params_list <- lapply(seq_len(n_multistart), function(i) {
    lower + (upper - lower) * lhs_samples[i, ]
  })
  
  for (init in init_params_list) {
    fit <- tryCatch({
      optimx::optimx(
        par = init,
        fn = function(par) { direct_learning_model2(par, participant_data) },
        method = "L-BFGS-B",
        lower = lower,
        upper = upper,
        control = list(maxit = 3000, parscale = upper - lower, dowarn = FALSE)
      )
    }, error = function(e) NULL)
    
    if (!is.null(fit)) {
      nll_val <- fit$value[1]
      if (nll_val < best_nll) {
        best_nll <- nll_val
        best_params <- as.numeric(fit[1, 1:6])
      }
    }
  }
  
  if (is.null(best_params)) {
    return(list(error = "Fitting failed"))
  }
  
  n_obs <- sum(participant_data$investment > 0, na.rm = TRUE)
  n_params <- length(best_params)
  aic <- 2 * best_nll + 2 * n_params
  bic <- 2 * best_nll + log(n_obs) * n_params
  
  list(params = best_params,
       neg_log_lik = best_nll,
       aic = aic,
       bic = bic)
}

```


```{r}
fit_direct_model2_single (test_data1,n_multistart = 2)
```

## Parallel fitting all participants

```{r}
# (b) Fit the model for all participants (full data) in parallel.
fit_all_participants_direct_model2 <- function(full_data, n_multistart = 20, n_cores = 8) {
  participants <- split(full_data, full_data$playerId)
  
  cl <- makeCluster(n_cores)
  clusterExport(cl, varlist = c("direct_learning_model2", "fit_direct_model2_single",
                                "get_investment_bin", "get_return_bin", "calculate_fs_utility"),
                envir = environment())
  clusterEvalQ(cl, { library(tidyverse); library(optimx); library(lhs) })
  
  results_list <- parLapplyLB(cl, participants, function(p_data) {
    tryCatch({
      fit_direct_model2_single(p_data, n_multistart = n_multistart)
    }, error = function(e) list(error = e$message))
  })
  stopCluster(cl)
  
  results_df <- do.call(rbind, lapply(names(results_list), function(pid) {
    fit <- results_list[[pid]]
    if (!is.null(fit$error)) {
      data.frame(playerId = pid, neg_log_lik = NA, aic = NA, bic = NA, error = fit$error,
                 stringsAsFactors = FALSE)
    } else {
      data.frame(playerId = pid,
                 neg_log_lik = fit$neg_log_lik,
                 aic = fit$aic,
                 bic = fit$bic,
                 error = NA,
                 stringsAsFactors = FALSE)
    }
  }))
  
  results_df
}


```

```{r}
# Fit all participants with all available data 
cat("\nSingle fit successful, proceeding with parallel fitting...\n")
results_simpleRL <- fit_all_participants_direct_model2(final_data, n_multistart = 5)

# Save results
write.csv(results_simpleRL, "results_simpleRL.csv", row.names = FALSE)

```




# Out of sample prediction and testing 



```{r}

###############################################################################
# 4B) Out–of–Sample Prediction Functions (Training/Test Split)
###############################################################################
# (a) Simulation function for a single game using direct_learning_model2 update rules.
simulate_direct_model2_game <- function(game_data, params, Q_init = NULL) {
  # params is a vector: [alpha_game1, alpha_game2, temp_game1, temp_game2, envy, guilt]
  alpha_game1 <- params[1]
  alpha_game2 <- params[2]
  temp_game1  <- params[3]
  temp_game2  <- params[4]
  envy        <- params[5]
  guilt       <- params[6]
  gamma       <- 1
  
  n_trials <- nrow(game_data)
  nll <- 0
  predictions <- data.frame(round = integer(), observed_bin = integer(),
                            predicted_bin = integer(), predicted_prob = numeric())
  n_actions <- 6
  
  # Initialize Q–values if not provided.
  if (is.null(Q_init)) {
    Q_values <- matrix(0, nrow = 3, ncol = n_actions)
  } else {
    Q_values <- Q_init
  }
  
  # Assume game_data contains a single game.
  current_game <- unique(game_data$gameNum.f)
  if (length(current_game) != 1) 
    stop("simulate_direct_model2_game: game_data must contain one game")
  
  for (t in seq_len(n_trials)) {
    investment <- game_data$investment[t]
    actual_return <- game_data$return[t]
    if (is.na(investment) || is.na(actual_return) || investment == 0) next
    
    state <- get_investment_bin(investment)
    # Choose parameters based on game.
    if (current_game == "first game") {
      alpha <- alpha_game1
      temp  <- temp_game1
    } else {
      alpha <- alpha_game2
      temp  <- temp_game2
    }
    
    # ------------------- Prediction Step (Before Q–value update) -------------------
    current_Q <- Q_values[state, ]
    current_Q_shift <- current_Q - max(current_Q)
    probs <- exp(current_Q_shift / temp)
    probs <- probs / sum(probs)
    probs <- pmax(probs, 1e-10)
    probs <- probs / sum(probs)
    
    actual_prop <- actual_return / (3 * investment)
    observed_action <- get_return_bin(actual_prop)
    predicted_action <- which.max(probs)
    
    predictions <- rbind(predictions,
                         data.frame(round = t,
                                    observed_bin = observed_action,
                                    predicted_bin = predicted_action,
                                    predicted_prob = probs[observed_action]))
    nll <- nll - log(probs[observed_action])
    
    # -------------------- Learning/Update Step (After prediction) --------------------
    trustee_payoff <- 3 * investment - actual_return
    investor_payoff <- actual_return - investment
    reward <- calculate_fs_utility(trustee_payoff, investor_payoff, envy, guilt)
    
    if (t < n_trials) {
      next_invest <- game_data$investment[t + 1]
      if (!is.na(next_invest) && next_invest > 0) {
        next_state <- get_investment_bin(next_invest)
        future_val <- max(Q_values[next_state, ])
      } else {
        future_val <- 0
      }
    } else {
      future_val <- 0
    }
    
    pe <- reward + gamma * future_val - Q_values[state, observed_action]
    Q_values[state, observed_action] <- Q_values[state, observed_action] + alpha * pe
  }
  
  list(nll = nll, Q_values = Q_values, predictions = predictions)
}

# (b) For each game in the training data, simulate the game to obtain final Q–values.
get_training_Q_direct_model2 <- function(training_data, params) {
  games <- unique(training_data$gameNum.f)
  Q_list <- list()
  for (g in games) {
    game_data <- training_data %>% filter(gameNum.f == g)
    sim_result <- simulate_direct_model2_game(game_data, params)
    Q_list[[g]] <- sim_result$Q_values
  }
  Q_list
}

# (c) For test data, simulate each game starting from the corresponding training Q–values.
simulate_test_direct_model2 <- function(test_data, params, training_Q_list) {
  games <- unique(test_data$gameNum.f)
  total_nll <- 0
  details <- list()
  for (g in games) {
    game_data <- test_data %>% filter(gameNum.f == g)
    if (is.null(training_Q_list[[g]])) {
      warning(paste("No training Q–values found for game", g))
      next
    }
    Q_init <- training_Q_list[[g]]
    sim_result <- simulate_direct_model2_game(game_data, params, Q_init = Q_init)
    total_nll <- total_nll + sim_result$nll
    details[[g]] <- sim_result
  }
  list(total_nll = total_nll, details = details)
}

# (d) Participant–level function for out–of–sample prediction.
fit_and_test_participant_direct_model2 <- function(participant_data, n_multistart = 20) {
  # Create a round index if not already present.
  if (!"roundNum" %in% names(participant_data)) {
    participant_data <- participant_data %>%
      group_by(gameNum.f) %>% 
      mutate(roundNum = row_number()) %>% 
      ungroup()
  }
  training_data <- participant_data %>% filter(roundNum <= 20)
  test_data <- participant_data %>% filter(roundNum > 20)
  
  fit_results <- tryCatch({
    fit_direct_model2_single(training_data, n_multistart = n_multistart)
  }, error = function(e) {
    cat("Fitting error for participant", unique(participant_data$playerId), ":", e$message, "\n")
    return(NULL)
  })
  if (is.null(fit_results) || !is.null(fit_results$error)) {
    return(list(playerId = unique(participant_data$playerId),
                fit_error = ifelse(is.null(fit_results), "Fitting failed", fit_results$error),
                test_nll = NA,
                bin_accuracy = NA,
                parameters = NA))
  }
  
  params <- fit_results$params
  
  training_Q_list <- get_training_Q_direct_model2(training_data, params)
  test_sim <- simulate_test_direct_model2(test_data, params, training_Q_list)
  
  all_predictions <- do.call(rbind, lapply(test_sim$details, function(x) x$predictions))
  if (!is.null(all_predictions) && is.data.frame(all_predictions) && nrow(all_predictions) > 0) {
    bin_accuracy <- mean(all_predictions$observed_bin == all_predictions$predicted_bin)
  } else {
    bin_accuracy <- NA
  }
  
  list(playerId = unique(participant_data$playerId),
       fit_error = NA,
       test_nll = test_sim$total_nll,
       bin_accuracy = bin_accuracy,
       parameters = params,
       training_fit_nll = fit_results$neg_log_lik)
}
```


```{r}
fit_and_test_participant_direct_model2(test_data1, n_multistart = 2) 
```


```{r}
# (e) Run out–of–sample prediction for all participants in parallel.
fit_and_test_all_participants_direct_model2 <- function(final_data, n_multistart = 20, n_cores = 8) {
  if (!"roundNum" %in% names(final_data)) {
    final_data <- final_data %>%
      group_by(gameNum.f, playerId) %>% 
      mutate(roundNum = row_number()) %>% 
      ungroup()
  }
  
  participants <- split(final_data, final_data$playerId)
  
  cl <- makeCluster(n_cores)
  clusterExport(cl, varlist = c("direct_learning_model2", "fit_direct_model2_single",
                                "simulate_direct_model2_game", "get_training_Q_direct_model2",
                                "simulate_test_direct_model2", "fit_and_test_participant_direct_model2",
                                "get_investment_bin", "get_return_bin", "calculate_fs_utility"),
                envir = environment())
  clusterEvalQ(cl, { library(tidyverse); library(optimx); library(lhs) })
  
  results_list <- parLapplyLB(cl, participants, function(p_data) {
    tryCatch({
      fit_and_test_participant_direct_model2(p_data, n_multistart = n_multistart)
    }, error = function(e) {
      list(playerId = unique(p_data$playerId),
           fit_error = e$message,
           test_nll = NA,
           bin_accuracy = NA,
           parameters = NA)
    })
  })
  stopCluster(cl)
  
  results_df <- do.call(rbind, lapply(results_list, function(res) {
    data.frame(playerId = res$playerId,
               fit_error = ifelse(is.null(res$fit_error), NA, res$fit_error),
               test_nll = res$test_nll,
               bin_accuracy = res$bin_accuracy,
               training_fit_nll = res$training_fit_nll,
               stringsAsFactors = FALSE)
  }))
  
  results_df
}

```




```{r}
# For example, to run parallel fitting and testing on all participants using 8 cores:
results_MFRL_test <- fit_and_test_all_participants_direct_model2(final_data, n_multistart=5)

# Display the results:
print(results_MFRL_test)

write.csv(results_MFRL_test, "results_MFRL_test.csv", row.names = FALSE)
```




# Parameter recovery 

```{r}
updateState <- function(state, PnL) {
  if (state == "unhappy") {
    mod <- c(
      exp(0.0 + 0.0 * PnL),
      exp(-3.366027  + 0.40910797 * PnL),
      exp(-3.572619  - 0.08137274 * PnL)
    )
  } else if (state == "neutral") {
    mod <- c(
      exp(0.0 + 0.0 * PnL),
      exp(3.3142637  + 0.3763408  * PnL),
      exp(0.9169736  + 0.4502838  * PnL)
    )
  } else if (state == "happy") {
    mod <- c(
      exp(0.0 + 0.0 * PnL),
      exp(0.7134085  + 0.02101626 * PnL),
      exp(2.2215478  + 0.16162964 * PnL)
    )
  } else {
    stop("updateState: unknown current state.")
  }
  mod <- mod / sum(mod)
  new_states <- c("unhappy","neutral","happy")
  return(new_states[sampleCat(mod)])
}

updateState_vol <- function(state, PnL) {
  if (state == "unhappy") {
    mod <- c(
      exp(0.0 + 0.0 * PnL),
      exp(-3.366027  + 0.40910797 * PnL),
      exp(-3.572619  - 0.08137274 * PnL)
    )
  } else if (state == "neutral") {
    mod <- c(
      exp(0.0 + 0.0 * PnL),
      exp(1 + 0.27 * PnL),
      exp(-4 + 0.75 * PnL)
    )
  } else if (state == "happy") {
    mod <- c(
      exp(0.0 + 0.0 * PnL),
      exp(0.7134085  + 0.02101626 * PnL),
      exp(2.2215478  + 0.16162964 * PnL)
    )
  } else {
    stop("updateState_vol: unknown current state.")
  }
  mod <- mod / sum(mod)
  new_states <- c("unhappy","neutral","happy")
  return(new_states[sampleCat(mod)])
}

# Sample from a categorical distribution given probability vector 'prob'
sampleCat <- function(prob) {
  sample(seq_along(prob), size = 1, prob = prob)
}
```


```{r}
###############################################################################
# 3) Simulation Function: simulate_2games_data_mf_hmm
###############################################################################
# Now using 6 trustee parameters:
#   c(alpha_game1, alpha_game2, temp_game1, temp_game2, envy, guilt)
simulate_2games_data_mf_hmm <- function(params_trustee, game_size = 25, playerId = 1) {
  if (length(params_trustee) != 6) {
    stop("simulate_2games_data_mf_hmm expects 6 trustee parameters: alpha_game1, alpha_game2, temp_game1, temp_game2, envy, guilt")
  }
  alpha_game1 <- params_trustee[1]
  alpha_game2 <- params_trustee[2]
  temp_game1  <- params_trustee[3]
  temp_game2  <- params_trustee[4]
  envy        <- params_trustee[5]
  guilt       <- params_trustee[6]
  
  total_rounds <- 2 * game_size
  df <- data.frame(
    trial      = 1:total_rounds,
    investment = rep(NA, total_rounds),
    return     = rep(NA, total_rounds),
    gameNum.f  = factor(rep("first game", total_rounds),
                        levels = c("first game", "second game"))
  )
  df$gameNum.f[(game_size + 1):total_rounds] <- "second game"
  
  # Start each game in the "neutral" state.
  current_state <- "neutral"
  
  # Trustee Q-values: 3 states x 6 actions.
  Q <- matrix(0, nrow = 3, ncol = 6)
  gamma <- 1
  
  for (t in 1:total_rounds) {
    # Reinitialize Q and investor state at the start of game 2.
    if (t == (game_size + 1)) {
      Q <- matrix(0, nrow = 3, ncol = 6)
      current_state <- "neutral"
    }
    
    # 1) Determine investment from the current state.
    s_bin <- state_to_bin(current_state)
    invest_amt <- bin_to_investment(s_bin)
    df$investment[t] <- invest_amt
    
    # 2) Choose parameters based on current game.
    current_game <- as.character(df$gameNum.f[t])
    if (current_game == "first game") {
      alpha <- alpha_game1
      temp  <- temp_game1
    } else {
      alpha <- alpha_game2
      temp  <- temp_game2
    }
    
    # 3) Trustee selects action via softmax.
    Q_s <- Q[s_bin, ]
    centered <- Q_s - max(Q_s)
    exp_vals <- exp(centered / temp)
    if (!all(is.finite(exp_vals)) || sum(exp_vals) == 0) {
      probs <- rep(1/6, 6)
    } else {
      probs <- exp_vals / sum(exp_vals)
    }
    a_bin <- sample(1:6, 1, prob = probs)
    ret_prop <- bin_to_return_prop(a_bin)
    ret_amt  <- round(ret_prop * (3 * invest_amt))
    df$return[t] <- ret_amt
    
    # 4) Compute payoffs and reward.
    trustee_payoff  <- 3 * invest_amt - ret_amt
    investor_payoff <- ret_amt - invest_amt
    rew <- calculate_fs_utility(trustee_payoff, investor_payoff, envy, guilt)
    
    # 5) Update: compute TD error and update Q-values.
    if (t < total_rounds) {
      if (current_game == "first game") {
        updateFunc <- updateState
      } else {
        updateFunc <- updateState_vol
      }
      PnL <- ret_amt - invest_amt
      next_state <- updateFunc(current_state, PnL)
      s_bin_next <- state_to_bin(next_state)
      td_err <- rew + gamma * max(Q[s_bin_next, ]) - Q[s_bin, a_bin]
      Q[s_bin, a_bin] <- Q[s_bin, a_bin] + alpha * td_err
      
      # Transition to next state.
      current_state <- next_state
    } else {
      td_err <- rew - Q[s_bin, a_bin]
      Q[s_bin, a_bin] <- Q[s_bin, a_bin] + alpha * td_err
    }
  }
  
  df$playerId <- playerId
  return(df)
}
```

```{r}
###############################################################################
# 4) Parameter Recovery Experiment
###############################################################################
param_recovery_experiment_mf <- function(n_sims = 10, game_size = 25) {
  results <- data.frame()
  
  for (i in seq_len(n_sims)) {
    # Sample trustee parameters.
    alpha_game1 <- runif(1, 0.01, 0.99)
    alpha_game2 <- runif(1, 0.01, 0.99)
    temp_game1  <- runif(1, 0.1, 5.0)
    temp_game2  <- runif(1, 0.1, 5.0)
    envy        <- runif(1, 0.0,  2.0)
    guilt       <- runif(1, 0.0,  1.5)
    params_true <- c(alpha_game1, alpha_game2, temp_game1, temp_game2, envy, guilt)
    
    # Simulate data.
    sim_data <- simulate_2games_data_mf_hmm(params_true, game_size, playerId = i)
    
    # Fit the model.
    fit_res <- fit_direct_model2_single(sim_data, n_multistart = 5)
    
    # Store the recovered parameters.
    if (!is.null(fit_res$params) && is.null(fit_res$error)) {
      fitted_par <- fit_res$params
      df_one <- data.frame(
        sim_id           = i,
        true_alpha_game1 = alpha_game1,
        fit_alpha_game1  = fitted_par[1],
        true_alpha_game2 = alpha_game2,
        fit_alpha_game2  = fitted_par[2],
        true_temp_game1  = temp_game1,
        fit_temp_game1   = fitted_par[3],
        true_temp_game2  = temp_game2,
        fit_temp_game2   = fitted_par[4],
        true_envy        = envy,
        fit_envy         = fitted_par[5],
        true_guilt       = guilt,
        fit_guilt        = fitted_par[6],
        neg_log_lik      = fit_res$neg_log_lik
      )
      results <- rbind(results, df_one)
    } else {
      df_fail <- data.frame(sim_id = i, error = "Fitting failed")
      results <- rbind(results, df_fail)
    }
  }
  return(results)
}
```



```{r}
recover_res_mf <- param_recovery_experiment_mf(n_sims=200)
recover_res_mf 

write.csv(recover_res_mf, "recover_res_mf.csv")
```


```{r}
# Load necessary libraries
library(dplyr)
library(ggplot2)

# Function to analyze parameter recovery for the model-free Q-learning model.
analyze_recovery_2games_mf <- function(results_df) {
  # Filter out participants with missing fitted parameters.
  good_df <- results_df %>% filter(!is.na(fit_envy))
  
  # List of parameter pairs: each element is a vector of (true_param, fit_param)
  param_pairs <- list(
    c("true_envy",  "fit_envy"),
    c("true_guilt", "fit_guilt"),
    c("true_temp_game1",  "fit_temp_game1"),
    c("true_temp_game2",  "fit_temp_game2"),
    c("true_alpha_game1", "fit_alpha_game1"),
    c("true_alpha_game2", "fit_alpha_game2")
  )
  
  # Compute and print correlation and MSE for each parameter pair.
  for (pp in param_pairs) {
    pt <- pp[1]
    pf <- pp[2]
    cor_val <- cor(good_df[[pt]], good_df[[pf]], use = "complete.obs")
    diffs   <- good_df[[pt]] - good_df[[pf]]
    mse_val <- mean(diffs^2, na.rm = TRUE)
    cat(sprintf("%s vs %s => r = %.3f, MSE = %.3f\n", pt, pf, cor_val, mse_val))
  }
}

# Example usage:
analyze_recovery_2games_mf(recover_res_mf)
```

```{r}
# ---- Plotting the Results of Parameter Recovery ----

library(dplyr)
library(ggplot2)

# Define the parameter pairs
param_pairs <- list(
  c("true_envy",         "fit_envy"),
  c("true_guilt",        "fit_guilt"),
  c("true_temp_game1",   "fit_temp_game1"),
  c("true_temp_game2",   "fit_temp_game2"),
  c("true_alpha_game1",  "fit_alpha_game1"),
  c("true_alpha_game2",  "fit_alpha_game2")
)

# Build a "long" data frame with one row per parameter per participant.
df_plot <- do.call(rbind, lapply(param_pairs, function(pp) {
  true_name <- pp[1]
  fit_name  <- pp[2]
  # Remove the "true_" prefix for the base name
  base_name <- gsub("^true_", "", true_name)
  
  data.frame(
    param    = base_name,
    true_val = recover_res_mf[[true_name]],
    fit_val  = recover_res_mf[[fit_name]]
  )
}))

# Compute correlation for each parameter.
df_cor <- df_plot %>%
  group_by(param) %>%
  summarise(
    r = cor(true_val, fit_val, use = "complete.obs")
  )

# Join the correlations back to the long data frame.
df_plot <- df_plot %>%
  left_join(df_cor, by = "param")

# Create a scatterplot for each parameter showing the true vs. fitted values.
ggplot(df_plot, aes(x = true_val, y = fit_val)) +
  geom_point(alpha = 0.5) +
  facet_wrap(~ param, scales = "free") +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  geom_text(
    data = df_cor,
    aes(
      x = Inf,
      y = -Inf,
      label = paste0("r = ", round(r, 2))
    ),
    hjust = 1.1,
    vjust = -0.5,
    color = "blue"
  ) +
  theme_minimal() +
  labs(
    title = "True vs. Fitted Parameters (Model-free Q-Learning)",
    x = "True value",
    y = "Fitted value"
  )
```






