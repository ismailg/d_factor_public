---
title: "new_hierarch_modelbased_RL"
author: "Ismail Guennouni"
date: "`r Sys.Date()`"
output: html_document
---


# Q learning with opponents states and transition 

This model is a reinforcement learning (RL) model with inequality aversion and state transitions. It extends the basic Q-learning framework by incorporating Fehr-Schmidt inequality aversion and a Hidden Markov Model (HMM) inference for opponent state transitions. 

## Key Features of the RL + Inequality Aversion + HMM Model

This model extends the basic Q-learning framework to analyze behavior in trust games by incorporating **inequality aversion** and **state transitions**. Below are the key features:

### State Representation
- The investor's behavior is characterized by **three hidden states**, each associated with a fixed investment amount (`STATE_INVESTMENTS <- c(4, 11, 17)`).
- The trustee infers the current state of the investor based on the observed investment amount.

### Action Space
- Participants choose from a set of **return amounts** (e.g., returning 0, 1, 2, ..., up to the tripled investment amount).
- Return amounts are continuous but discretized into 6 bins for computational purposes.

### Learning Mechanism
- Participants learn **Q-values** for each state, representing the expected future rewards.
- Q-values are updated using a **temporal difference (TD) learning rule**, incorporating:
  - **Immediate rewards**: Calculated using the **Fehr-Schmidt utility function**, which accounts for inequality aversion (envy and guilt).
  - **Future rewards**: Discounted by the transition probabilities between states, which depend on the trustee's return behavior.
  
The Q-value is updated as:
  
  $$ Q(s) \leftarrow Q(s) + \alpha \left[ r_{\text{FS}} + \sum_{s'} P(s' | s, a) Q(s') - Q(s) \right] $$
  
  
Where:
- \( Q(s) \): Current Q-value for state \( s \).
- \( \alpha \): Learning rate.
- \( r_{\text{FS}} \): Immediate reward calculated using the Fehr-Schmidt utility function.
- \( P(s' | s, a) \): Transition probability to state \( s' \) given the current state \( s \) and action \( a \).
- \( \sum_{s'} P(s' | s, a) Q(s') \): Expected future reward, weighted by transition probabilities.


### State Transitions
- The model assumes that the investor's state transitions depend on the **net return** (return amount minus investment) provided by the trustee.
- Transition probabilities are calculated using a **sensitivity parameter**, which determines how strongly the net return affects state changes.

  $$
  P(S_{t+1}=s'|S_t=s,\Delta) = \frac{1}{1 + e^{-\alpha\Delta}} \quad (\Delta = 3p_tI_t - I_t)
  $$
  
  where $p_t$ is the midpoint of chosen bin $a_t$



### Decision Making
- Participants choose actions probabilistically using a **softmax function**, which balances exploration and exploitation based on a temperature parameter (`temp`).

### Model Fitting
- The model is fitted to participant data by maximizing the **log-likelihood** of observed choices under the softmax action selection rule.
- Parameters (`envy`, `guilt`, `temp`, `sensitivity`, `alpha_Q`) are estimated using optimization techniques (e.g., L-BFGS-B).

### Hierarchical Approach
- The model supports a **hierarchical Bayesian approach**, where parameters are constrained by literature-based priors (e.g., typical values for envy, guilt, and learning rates).



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Fitting to all data 

## Tools and helpful functions

```{r}
############################################################################
# 1) Libraries
############################################################################
library(tidyverse)
library(optimx)
library(lhs)
library(parallel)

############################################################################
# 2) Basic Helper Functions
############################################################################

# Convert investor's investment into 3 discrete bins: {1=Low, 2=Med, 3=High}
get_investment_bin <- function(investment) {
  if (investment <= 7)   return(1)  # Low
  if (investment <= 14)  return(2)  # Medium
  return(3)                        # High
}

# Convert trustee's return proportion (0..1) into 6 bins
get_return_bin <- function(return_prop) {
  if (return_prop < 0 || return_prop > 1) {
    stop("Return proportion must be between 0 and 1.")
  }
  # Bin numbers: 1..6 (with 1 representing 0 to <1/6, etc.)
  return(min(floor(return_prop * 6) + 1, 6))
}

# Calculate actual monetary payoffs for the trustee and investor
calculate_payoffs <- function(investment, return_amount) {
  trustee_payoff  <- 3 * investment - return_amount
  investor_payoff <- return_amount  - investment
  list(trustee = trustee_payoff, investor = investor_payoff)
}

# Fehr–Schmidt utility
calculate_fs_utility <- function(own_payoff, other_payoff, envy, guilt) {
  disadv_inequity <- max(other_payoff - own_payoff, 0)
  adv_inequity    <- max(own_payoff - other_payoff, 0)
  own_payoff - envy * disadv_inequity - guilt * adv_inequity
}

# Priors if needed (hierarchical or regularization)
get_literature_priors <- function() {
  list(
    means = c(
      envy = 2.0, guilt = 0.5, 
      temp_game1 = 5.0, temp_game2 = 5.0,
      sensitivity_game1 = 0.5, sensitivity_game2 = 0.5,
      alpha_Q_game1 = 0.3, alpha_Q_game2 = 0.3
    ),
    sds = c(
      envy = 1.0, guilt = 0.3,
      temp_game1 = 2.0, temp_game2 = 2.0,
      sensitivity_game1 = 0.3, sensitivity_game2 = 0.3,
      alpha_Q_game1 = 0.2, alpha_Q_game2 = 0.2
    )
  )
}

############################################################################
# 3) Forward Simulation Helpers (Bins -> Representative Values, Transitions)
############################################################################

# For forward-looking simulation:
bin_to_investment <- function(s_bin) {
  # Representative investment per bin.
  if (s_bin == 1) return(4)   # Low
  if (s_bin == 2) return(11)  # Medium
  if (s_bin == 3) return(17)  # High
  stop("Invalid s_bin in bin_to_investment()")
}

bin_to_return_prop <- function(a_bin) {
  # Representative return proportion from bin a_bin (1:6) as the middle of the bin
  return((2 * a_bin - 1) / 12)
}

# Immediate FS payoff for a given state-action pair
fs_payoff_state_action <- function(s_bin, a_bin, envy, guilt) {
  inv  <- bin_to_investment(s_bin)
  prop <- bin_to_return_prop(a_bin)
  ret  <- round(prop * (3 * inv))
  
  trustee_payoff  <- 3 * inv - ret
  investor_payoff <- ret - inv
  
  disadv_inequity <- max(investor_payoff - trustee_payoff, 0)
  adv_inequity    <- max(trustee_payoff - investor_payoff, 0)
  u <- trustee_payoff - envy * disadv_inequity - guilt * adv_inequity
  return(u)
}

# Transition probabilities for next state given current state-action.
transition_probs_state_action <- function(s_bin, a_bin, sensitivity) {
  inv  <- bin_to_investment(s_bin)
  prop <- bin_to_return_prop(a_bin)
  ret  <- prop * (3 * inv)
  net_return <- ret - inv
  
  probs <- rep(0, 3)
  if (s_bin == 1) {
    p_up      <- exp(sensitivity * net_return)
    p_stay    <- 1
    norm_const <- p_up + p_stay
    probs[1] <- 0                  # cannot go down from s=1
    probs[2] <- p_stay / norm_const
    probs[3] <- p_up   / norm_const
  } else if (s_bin == 3) {
    p_down     <- exp(-sensitivity * net_return)
    p_stay     <- 1
    norm_const <- p_down + p_stay
    probs[1]   <- p_down / norm_const
    probs[2]   <- p_stay / norm_const
    probs[3]   <- 0                # cannot go up from s=3
  } else {  # s_bin == 2
    p_up   <- exp(sensitivity * net_return)
    p_down <- exp(-sensitivity * net_return)
    p_stay <- 1
    norm_const <- p_up + p_down + p_stay
    probs[1] <- p_down / norm_const
    probs[2] <- p_stay / norm_const
    probs[3] <- p_up   / norm_const
  }
  return(probs)
}

############################################################################
# 4) k-Step Lookahead via Dynamic Programming (DP)
############################################################################
# Instead of recursion with caching, we can compute the k-step lookahead
# for all 18 (state, action) pairs by “backing up” the Bellman equation.
# That is, for each state s and action a, define:
#   R(s,a) = fs_payoff_state_action(s,a, envy, guilt)
#   P(s,a, s') = transition_probs_state_action(s,a, sensitivity)
#
# Then, for a planning depth of d = 1, we have:
#   V_1(s,a) = R(s,a) + sum_{s'} P(s,a,s') * max_{a'} Q_MB_current(s', a')
# For d > 1:
#   V_d(s,a) = R(s,a) + sum_{s'} P(s,a,s') * max_{a'} V_{d-1}(s', a')
#
# We return the matrix V (3×6) for the given planning depth.

compute_dp_value <- function(Q_MB_current, envy, guilt, sensitivity, plan_depth) {
  # Precompute immediate rewards R(s,a) and transition probabilities P(s,a,*)
  R <- matrix(0, nrow = 3, ncol = 6)
  # We will store the transition probabilities in a 3D array:
  # For each state (row) and action (col), we have a vector of length 3.
  P_array <- array(0, dim = c(3, 6, 3))
  for (s in 1:3) {
    for (a in 1:6) {
      R[s, a] <- fs_payoff_state_action(s, a, envy, guilt)
      P_array[s, a, ] <- transition_probs_state_action(s, a, sensitivity)
    }
  }
  
  # Base case: planning depth 1
  # For each (s,a): value = R(s,a) + sum_{s'} P(s,a,s') * max_{a'} Q_MB_current[s', a']
  max_Q <- apply(Q_MB_current, 1, max)  # vector of length 3
  dp <- matrix(0, nrow = 3, ncol = 6)
  for (s in 1:3) {
    for (a in 1:6) {
      dp[s, a] <- R[s, a] + sum(P_array[s, a, ] * max_Q)
    }
  }
  
  # For depths 2 up to plan_depth, iteratively backup.
  if (plan_depth >= 2) {
    for (d in 2:plan_depth) {
      max_dp <- apply(dp, 1, max)
      for (s in 1:3) {
        for (a in 1:6) {
          dp[s, a] <- R[s, a] + sum(P_array[s, a, ] * max_dp)
        }
      }
    }
  }
  return(dp)
}

############################################################################
# 5) Single-K Hybrid Model (Using DP Lookahead)
############################################################################

trustee_decision_MB_k <- function(params, data, plan_depth, use_priors = FALSE) {
  # Expect 9 parameters: envy, guilt, temp_game1, temp_game2, sensitivity_game1,
  # sensitivity_game2, alpha_Q_game1, alpha_Q_game2.
  if (length(params) != 8) {
    stop("Requires 8 parameters: envy, guilt, temp1, temp2, sens1, sens2, alpha1, alpha2")
  }
  
  # Extract parameters:
  envy              <- params[1]
  guilt             <- params[2]
  temp_game1        <- params[3]
  temp_game2        <- params[4]
  sensitivity_game1 <- params[5]
  sensitivity_game2 <- params[6]
  alpha_Q_game1     <- params[7]
  alpha_Q_game2     <- params[8]
  
  # Basic parameter checks:
  if (any(is.na(params)) || any(params < 0)) {
    return(1e10)
  }
  
  n_trials <- nrow(data)
  log_lik  <- 0
  
  # Initialize Q-values (3 states x 6 actions)
  Q_MB <- matrix(0, nrow = 3, ncol = 6)  # model-based Q
  
  current_game <- NULL
  
  for (t in seq_len(n_trials)) {
    investment <- data$investment[t]
    return_amt <- data$return[t]
    game_label <- data$gameNum.f[t]
    
    # Skip trials with missing/invalid data:
    if (investment <= 0) next
    return_prop <- return_amt / (3 * investment)
    if (return_prop < 0 || return_prop > 1) next
    
    # Reset Q values at game boundaries.
    if (!is.null(current_game) && game_label != current_game) {
      Q_MB <- matrix(0, nrow = 3, ncol = 6)
    }
    current_game <- game_label
    
    # Set game-specific parameters:
    if (game_label == "first game") {
      temp        <- temp_game1
      sensitivity <- sensitivity_game1
      alpha_Q     <- alpha_Q_game1
    } else {
      temp        <- temp_game2
      sensitivity <- sensitivity_game2
      alpha_Q     <- alpha_Q_game2
    }
    
    # Identify state and action bins from data:
    s_bin <- get_investment_bin(investment)
    a_bin <- get_return_bin(return_prop)
    
    # Compute the FS reward from the actual data:
    payoffs <- calculate_payoffs(investment, return_amt)
    reward  <- calculate_fs_utility(payoffs$trustee, payoffs$investor, envy, guilt)
    
    # 2) Model-Based update using DP lookahead:
    # Instead of a recursive call with caching, we compute the full 3×6 table.
    dp_values <- compute_dp_value(Q_MB, envy, guilt, sensitivity, plan_depth)
    val_k <- dp_values[s_bin, a_bin]
    td_err_MB <- val_k - Q_MB[s_bin, a_bin]
    Q_MB[s_bin, a_bin] <- Q_MB[s_bin, a_bin] + alpha_Q * td_err_MB
    
    # 3) Hybrid Action Selection via Softmax:
    Q_MB_state <-  Q_MB[s_bin, ]
    centered <- Q_MB_state - max(Q_MB_state)
    probs <- exp(centered / temp)
    probs <- probs / sum(probs)
    probs <- pmax(probs, 1e-10)
    
    log_lik <- log_lik + log(probs[a_bin])
  }
  
  # Apply priors if desired:
  if (use_priors) {
    priors <- get_literature_priors()
    prior_terms <- sum(dnorm(params,
                             mean = priors$means,
                             sd   = priors$sds,
                             log  = TRUE))
    return( - (log_lik + prior_terms) )
  }
  
  return( -log_lik )
}

```

## Fitting Function to single participant 

```{r}
############################################################################
# 6) Fitting Function That Tries k = 1..max_k
############################################################################

fit_participant_MB_k <- function(participant_data,
                                     max_k = 3,
                                     hierarchical = FALSE,
                                     n_multistart = 20) {
  # Parameter bounds for the 9 parameters:
  lower <- c(
    envy = 0.01, guilt = 0.01, 
    temp_game1 = 0.1, temp_game2 = 0.1,
    sensitivity_game1 = 0.01, sensitivity_game2 = 0.01,
    alpha_Q_game1 = 0.01, alpha_Q_game2 = 0.01
  )
  upper <- c(
    envy = 6, guilt = 2,
    temp_game1 = 15, temp_game2 = 15,
    sensitivity_game1 = 2, sensitivity_game2 = 2,
    alpha_Q_game1 = 0.99, alpha_Q_game2 = 0.99
  )
  
  set.seed(123)
  best_overall <- list(nll = Inf, k = NA, parameters = NULL)
  
  for (k_test in 1:max_k) {
    lhs_samples <- randomLHS(n_multistart, length(lower))
    init_params_list <- lapply(seq_len(n_multistart), function(i) {
      lower + (upper - lower) * lhs_samples[i, ]
    })
    
    fits_k <- lapply(init_params_list, function(start_par) {
      tryCatch({
        optimx(
          par     = start_par,
          fn      = function(par) {
            trustee_decision_MB_k(
              params     = par,
              data       = participant_data,
              plan_depth = k_test,
              use_priors = hierarchical
            )
          },
          method  = "L-BFGS-B",
          lower   = lower,
          upper   = upper,
          control = list(
            maxit    = 1000,
            parscale = upper - lower,
            dowarn   = FALSE
          )
        )
      }, error = function(e) NULL)
    })
    
    fits_k <- fits_k[!sapply(fits_k, is.null)]
    if (length(fits_k) == 0) next
    
    obj_values <- sapply(fits_k, function(x) x$value[1])
    best_idx   <- which.min(obj_values)
    best_fit_k <- fits_k[[best_idx]]
    
    if (!is.null(best_fit_k)) {
      best_params_k <- setNames(as.numeric(best_fit_k[1, 1:8]), names(lower))
      best_nll_k    <- best_fit_k$value[1]
      
      if (best_nll_k < best_overall$nll) {
        best_overall$nll <- best_nll_k
        best_overall$k <- k_test
        best_overall$parameters <- best_params_k
      }
    }
  }
  
  if (is.infinite(best_overall$nll)) {
    return(list(error = "All optimizations failed for all k"))
  }
  
  n_params <- length(best_overall$parameters)
  n_obs    <- sum(participant_data$investment > 0)
  best_overall$aic <- 2 * best_overall$nll + 2 * n_params
  best_overall$bic <- 2 * best_overall$nll + log(n_obs) * n_params
  
  return(best_overall)
}



```

```{r}
############################################################################
# 7) Example Test on One Participant’s Data
############################################################################


# Run fitting on test data (using max_k=2 for speed in testing)
test_results_mb_new <- fit_participant_MB_k(test_data1, max_k = 2, hierarchical = TRUE, n_multistart = 2)
cat("Estimated params:", test_results_mb_new$parameters, "\n")
```


## parallel fitting all participants

```{r}
############################################################################
# Load necessary libraries (including pbapply for progress reporting)
############################################################################
library(tidyverse)
library(optimx)
library(lhs)
library(parallel)
library(pbapply)   # For progress bar in parallel

############################################################################
# (Keep the rest of your functions here: helper functions, compute_dp_value,
#  trustee_decision_hybrid_k, etc.)
# Assume they are defined above this point.
############################################################################

############################################################################
# Parallel Fitting Over Participants with Progress Bar and Timing
############################################################################

fit_all_participants_mb_k <- function(data,
                                         max_k         = 3,
                                         hierarchical  = FALSE,
                                         n_multistart  = 50,
                                         n_cores       = 8) {
  # Check for required columns
  if (!"gameNum.f" %in% names(data)) stop("'gameNum.f' column is missing in the data.")
  if (!"playerId" %in% names(data)) stop("'playerId' column is missing in the data.")
  
  # Split data by participant
  participants <- split(data, data$playerId)
  n_participants <- length(participants)
  if (n_participants == 0) stop("No participants found in data.")
  
  # Set up the cluster
  cl <- makeCluster(n_cores)
  
  # Export the necessary objects/functions to the cluster.
  clusterExport(cl, c(
    "fit_participant_MB_k", 
    "trustee_decision_MB_k", 
    "compute_dp_value",
    "calculate_payoffs",
    "calculate_fs_utility",
    "get_investment_bin",
    "get_return_bin",
    "get_literature_priors",
    "bin_to_investment",
    "bin_to_return_prop",
    "fs_payoff_state_action",
    "transition_probs_state_action"
  ))
  
  # Load required packages on each node.
  clusterEvalQ(cl, {
    library(optimx)
    library(lhs)
  })
  
  # Record the start time.
  start_time <- Sys.time()
  message("Fitting ", n_participants, " participants...")
  
  # Use pbapply's progress bar version of parLapplyLB. The progress bar
  # will print the index of the participant being processed.
  results <- pblapply(seq_along(participants), function(i) {
    # Print a message for each participant (this will appear in the pbapply bar)
    message("Fitting participant ", i, " of ", n_participants)
    p_data <- participants[[i]]
    # Each worker calls the k-search fitter
    fit_result <- tryCatch(
      {
        fit_participant_MB_k(
          participant_data = p_data,
          max_k           = max_k,
          hierarchical    = hierarchical,
          n_multistart    = n_multistart
        )
      },
      error = function(e) {
        warning(paste("Error fitting participant:", e$message))
        return(NULL)
      }
    )
    fit_result
  }, cl = cl)
  
  # Stop the cluster
  stopCluster(cl)
  
  # Record the end time and print elapsed time.
  end_time <- Sys.time()
  elapsed <- difftime(end_time, start_time, units = "secs")
  message("Parallel fitting complete. Total elapsed time: ", elapsed, " seconds.")
  
  # Combine results into a data.frame.
  results_df <- do.call(rbind, lapply(seq_along(participants), function(i) {
    pid <- names(participants)[i]
    fit <- results[[i]]
    if (is.null(fit) || is.null(fit$parameters) || all(is.na(fit$parameters))) {
      return(data.frame(
        participant = pid,
        envy = NA,
        guilt = NA,
        temp_game1 = NA,
        temp_game2 = NA,
        sensitivity_game1 = NA,
        sensitivity_game2 = NA,
        alpha_Q_game1 = NA,
        alpha_Q_game2 = NA,
        nll = NA,
        aic = NA,
        bic = NA,
        best_k = NA,
        error = "Fitting failed",
        approach = ifelse(hierarchical, "hierarchical", "standard"),
        stringsAsFactors = FALSE
      ))
    }
    data.frame(
      participant = pid,
      envy = fit$parameters[["envy"]],
      guilt = fit$parameters[["guilt"]],
      temp_game1 = fit$parameters[["temp_game1"]],
      temp_game2 = fit$parameters[["temp_game2"]],
      sensitivity_game1 = fit$parameters[["sensitivity_game1"]],
      sensitivity_game2 = fit$parameters[["sensitivity_game2"]],
      alpha_Q_game1 = fit$parameters[["alpha_Q_game1"]],
      alpha_Q_game2 = fit$parameters[["alpha_Q_game2"]],
      nll = fit$nll,
      aic = fit$aic,
      bic = fit$bic,
      best_k = fit$k,
      approach = ifelse(hierarchical, "hierarchical", "standard"),
      stringsAsFactors = FALSE
    )
  }))
  
  return(results_df)
}


```

```{r}
############################################################################
# Example of Running the Fitting Function for One Participant
############################################################################
# Assume final_data is defined and has the required columns.

#Now get test data and fit model
first_2_participants <- unique(final_data$playerId)[1:2]
test_data2 <- final_data %>%
    dplyr::filter(playerId %in% first_2_participants)


# Run the fitting (for example with max_k = 2)
test_results_mb_new <- fit_all_participants_mb_k(test_data2, max_k = 2, hierarchical = TRUE, n_multistart = 2)
print(test_results_mb_new)
```

```{r}
# Run the hierarchical fitting
results_mb_k <- fit_all_participants_mb_k(final_data, max_k=3, hierarchical = TRUE, n_multistart=5)


# Print and summarize results
print(results_mb_k)
summary(attr(results_mb_k, "summary"))
write.csv(results_mb_k, "results_mb_k.csv", row.names = FALSE)
```


```{r}
library(ggplot2)

# Convert parameters to long format for faceted plotting
df_long <- results_mb_k %>%
  dplyr::select(-c("participant", "nll", "aic","bic","approach")) %>%
  pivot_longer(everything(), names_to = "parameter", values_to = "value")

# Plot distributions
ggplot(df_long, aes(x = value)) +
  geom_histogram(bins = 20, fill = "skyblue", color = "black") +
  facet_wrap(~ parameter, scales = "free") +
  labs(title = "Parameter Distributions Across Participants",
       x = "Value", y = "Frequency") +
  theme_minimal()
```

# Out of sample predictions 

```{r}
############################################################################
# 7) Simulation for a Single Game (Test Phase)
############################################################################
# This version also records the model’s predicted bin per round.
simulate_game_MB <- function(game_data, params, plan_depth, Q_MB_init = NULL) {
  if (is.null(Q_MB_init)) Q_MB <- matrix(0, nrow = 3, ncol = 6) else Q_MB <- Q_MB_init
  
  n_trials <- nrow(game_data)
  nll <- 0
  predictions <- data.frame(round = integer(), observed_bin = integer(),
                            predicted_bin = integer(), predicted_prob = numeric())
  
  game_label <- unique(game_data$gameNum.f)
  if (length(game_label) > 1) stop("simulate_game: game_data contains multiple games.")
  
  if (game_label == "first game") {
    temp <- params["temp_game1"]
    sensitivity <- params["sensitivity_game1"]
    alpha_Q <- params["alpha_Q_game1"]
  } else {
    temp <- params["temp_game2"]
    sensitivity <- params["sensitivity_game2"]
    alpha_Q <- params["alpha_Q_game2"]
  }
  
  for (t in seq_len(n_trials)) {
    investment <- game_data$investment[t]
    return_amt <- game_data$return[t]
    if (investment <= 0) next
    return_prop <- return_amt / (3 * investment)
    if (return_prop < 0 || return_prop > 1) next
    
    s_bin <- get_investment_bin(investment)
    a_bin <- get_return_bin(return_prop)
    
    # --- Prediction Step: Compute predicted probabilities BEFORE updating Q_MB ---
    Q_hybrid <- Q_MB[s_bin, ]
    centered <- Q_hybrid - max(Q_hybrid)
    probs <- exp(centered / temp)
    probs <- probs / sum(probs)
    probs <- pmax(probs, 1e-10)
    
    predicted_bin <- which.max(probs)
    nll <- nll - log(probs[a_bin])
    predictions <- rbind(predictions,
                         data.frame(round = t, observed_bin = a_bin,
                                    predicted_bin = predicted_bin,
                                    predicted_prob = probs[a_bin]))
    
    # --- Learning Step: Now update Q_MB based on the observed outcome ---
    payoffs <- calculate_payoffs(investment, return_amt)
    reward <- calculate_fs_utility(payoffs$trustee, payoffs$investor, 
                                   params["envy"], params["guilt"])
    
    dp_values <- compute_dp_value(Q_MB, params["envy"], params["guilt"], sensitivity, plan_depth)
    val_k <- dp_values[s_bin, a_bin]
    td_err_MB <- val_k - Q_MB[s_bin, a_bin]
    Q_MB[s_bin, a_bin] <- Q_MB[s_bin, a_bin] + alpha_Q * td_err_MB
  }
  
  list(nll = nll, Q_MB = Q_MB, predictions = predictions)
}

############################################################################
# 8) Extracting Final Q's from Training Data (Per Game)
############################################################################

get_training_Q <- function(training_data, params, plan_depth) {
  games <- unique(training_data$gameNum.f)
  Q_list <- list()
  for (g in games) {
    game_data <- training_data %>% filter(gameNum.f == g)
    sim_result <- simulate_game_MB(game_data, params, plan_depth)
    Q_list[[g]] <- list(Q_MF = sim_result$Q_MF, Q_MB = sim_result$Q_MB)
  }
  Q_list
}

############################################################################
# 9) Simulation for Test Data (Per Participant)
############################################################################

simulate_test_data <- function(test_data, params, plan_depth, training_Q_list) {
  games <- unique(test_data$gameNum.f)
  total_nll <- 0
  details <- list()
  for (g in games) {
    game_data <- test_data %>% filter(gameNum.f == g)
    if (is.null(training_Q_list[[g]])) {
      warning(paste("No training Q values found for game", g))
      next
    }
    Q_init <- training_Q_list[[g]]
    sim_result <- simulate_game_MB(game_data, params, plan_depth,
                                Q_MB_init = Q_init$Q_MB)
    total_nll <- total_nll + sim_result$nll
    details[[g]] <- sim_result
  }
  list(total_nll = total_nll, details = details)
}


```


## Fitting and Testing for a Single Participant

```{r}
############################################################################
# 10) Fitting and Testing for a Single Participant
############################################################################
# This function splits the participant's data into training (rounds 1–20) 
# and test (rounds >20), fits the model on training, then simulates the test 
# rounds to get the out-of-sample nll and a bin–accuracy measure.
fit_and_test_participant_MB <- function(participant_data, max_k = 3, hierarchical = TRUE, n_multistart = 20) {

  training_data <- participant_data %>% filter(roundNum <= 20)
  test_data     <- participant_data %>% filter(roundNum > 20)
  
  fit_results <- fit_participant_MB_k(training_data, max_k = max_k, hierarchical = hierarchical, n_multistart = n_multistart)
  
  if (!is.null(fit_results$error)) {
    return(list(playerId = unique(participant_data$playerId),
                fit_error = fit_results$error,
                test_nll = NA,
                bin_accuracy = NA,
                parameters = NA,
                best_k = NA))
  }
  
  training_Q_list <- get_training_Q(training_data, fit_results$parameters, plan_depth = fit_results$k)
  
  test_sim <- simulate_test_data(test_data, fit_results$parameters, plan_depth = fit_results$k, training_Q_list = training_Q_list)
  
  # Safely combine the predictions from all games (if any)
  predictions_list <- lapply(test_sim$details, function(x) x$predictions)
  if (length(predictions_list) > 0 && !all(sapply(predictions_list, is.null))) {
    all_predictions <- do.call(rbind, predictions_list)
    print(all_predictions)
    bin_accuracy <- if (nrow(all_predictions) > 0) mean(all_predictions$observed_bin == all_predictions$predicted_bin) else NA

  } else {
    bin_accuracy <- NA
  }
  
  list(playerId = unique(participant_data$playerId),
       fit_error = NA,
       test_nll = test_sim$total_nll,
       bin_accuracy = bin_accuracy,
       parameters = fit_results$parameters,
       best_k = fit_results$k)
}

```

```{r}

test_data_rand <- final_data %>% filter(playerId %in% unique(final_data$playerId)[70])
fit_and_test_participant_MB(test_data_rand, max_k = 2, hierarchical = TRUE, n_multistart = 2)
```

## Parallel Fitting and Testing for All Participants

```{r}
############################################################################
# 11) Parallel Fitting and Testing for All Participants
############################################################################
# This function splits final_data by participant, then in parallel fits and tests each one.
fit_and_test_all_participants_MB <- function(final_data, max_k = 3, hierarchical = TRUE, n_multistart = 20, n_cores = 4) {
  if (!"playerId" %in% names(final_data)) stop("final_data must have a playerId column")
  
  # if (!"round" %in% names(final_data)) {
  #   final_data <- final_data %>%
  #     group_by(gameNum.f, playerId) %>% mutate(round = row_number()) %>% ungroup()
  # }
  
  participants <- split(final_data, final_data$playerId)
  
  cl <- makeCluster(n_cores)
  clusterExport(cl, varlist = c("fit_participant_MB_k", "trustee_decision_MB_k",
                                "compute_dp_value", "calculate_payoffs", "calculate_fs_utility",
                                "get_investment_bin", "get_return_bin", "get_literature_priors",
                                "bin_to_investment", "bin_to_return_prop", "fs_payoff_state_action",
                                "transition_probs_state_action", "simulate_game_MB", "get_training_Q",
                                "simulate_test_data", "fit_and_test_participant_MB"),
                envir = environment())
  clusterEvalQ(cl, { library(tidyverse); library(optimx); library(lhs) })
  
  results_list <- parLapplyLB(cl, participants, function(p_data) {
    tryCatch({
      fit_and_test_participant_MB(p_data, max_k = max_k, hierarchical = hierarchical, n_multistart = n_multistart)
    }, error = function(e) {
      list(playerId = unique(p_data$playerId),
           fit_error = e$message,
           test_nll = NA,
           bin_accuracy = NA,
           parameters = NA,
           best_k = NA)
    })
  })
  stopCluster(cl)
  
  results_df <- do.call(rbind, lapply(results_list, function(res) {
    data.frame(playerId = res$playerId,
               fit_error = ifelse(is.null(res$fit_error), NA, res$fit_error),
               test_nll = res$test_nll,
               bin_accuracy = res$bin_accuracy,
               best_k = res$best_k,
               stringsAsFactors = FALSE)
  }))
  
  results_df
}

```



```{r}
# For example, to run parallel fitting and testing on all participants using 8 cores:
results_mbPlan_test <- fit_and_test_all_participants_MB(final_data, max_k = 3, hierarchical = TRUE, n_multistart = 3, n_cores = 8)

# Display the results:
print(results_mbPlan_test)

write.csv(results_mbPlan_test,"results_mbPlan_test.csv")
```



```{r}
library(dplyr)
library(tidyr)

# Reshape the model parameters from wide to long
analysis_data_long <- results_mb_k %>%
  # pivot the game-specific columns into long format:
  pivot_longer(
    cols = c("temp_game1", "temp_game2",
             "sensitivity_game1", "sensitivity_game2",
             "alpha_Q_game1", "alpha_Q_game2"),
    names_to   = c("param_type", "game_label"),
    names_pattern = "(.*)_(game\\d+)",  # captures (param)_(game1 or game2)
    values_to  = "param_value"
  ) %>%
  # now pivot them back out so we get columns temp, sensitivity, alpha_Q etc.
  pivot_wider(
    names_from  = "param_type",
    values_from = "param_value"
  ) %>%
  # rename the "game_label" into a factor matching your data
  mutate(
    gameNum.f = case_when(
      game_label == "game1" ~ "first game",
      game_label == "game2" ~ "second game"
    )
  )

```

```{r}
# Merge with payoff/opponent data
analysis_data_long <- analysis_data_long %>%
  left_join(
    payoff_data, 
    by = c("participant" = "playerId", "gameNum.f" = "gameNum.f")
  )


# Compare parameters between “AI_HMM” vs. “AI_HMM_vol”:
analysis_data_long %>%
  group_by(opponent.f) %>%
  summarize(
    mean_temp   = mean(temp, na.rm=TRUE),
    mean_sens   = mean(sensitivity, na.rm=TRUE),
    mean_alpha  = mean(alpha_Q, na.rm=TRUE)
  )
```

```{r}
library(afex)

# Model temp as a function of (opponent.f, d_level, etc.), random intercept by participant
mod_temp <- afex::mixed(
  temp ~ opponent.f*d_level + (1 | participant), 
  data = analysis_data_long
)
summary(mod_temp)

```

```{r}


# Model alpha as a function of (opponent.f, d_level, etc.), random intercept by participant
mod_alphaQ <- afex::mixed(
  alpha_Q ~ opponent.f*d_level + (1 | participant), 
  data = analysis_data_long
)
summary(mod_alphaQ)

```

```{r}
# global parameters 
analysis_data_global <- results_mb_k %>%
  select(participant, envy, guilt,  nll, aic, bic, best_k)


payoff_summary_participant <- payoff_data %>%
  group_by(playerId, d_level, vin_scaled, cal_scaled, dec_scaled, sad_scaled) %>%
  summarize(
    mean_payoff  = mean(payoff, na.rm = TRUE),
    .groups = "drop"
  )

analysis_data_global <- analysis_data_global %>%
  left_join(
    payoff_summary_participant, 
    by = c("participant" = "playerId")
  )
```



```{r}
library(ggplot2)
library(ggcorrplot)

vars_of_interest <- analysis_data_global %>%
  dplyr::select(envy, guilt,  mean_payoff, cal_scaled, dec_scaled, sad_scaled, vin_scaled)



```


```{r}

model_envy <- lm(
  envy ~ d_level + cal_scaled + dec_scaled + sad_scaled + vin_scaled,
  data = analysis_data_global
)
summary(model_envy)

```

```{r}
model_guilt <- lm(
  guilt ~ cal_scaled + dec_scaled + sad_scaled + vin_scaled ,
  data = analysis_data_global
)
summary(model_guilt)

library(car)
vif(model_guilt)  # If VIF > 5, consider removing correlated predictors

```




