---
title: "Parameter Sensitivity Analysis for MFRL Model"
author: "Ismail Guennouni"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: cosmo
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE,
                     fig.width = 10, fig.height = 6)
```

# 1. Introduction

This document implements a systematic parameter sensitivity analysis for the Model-Free Reinforcement Learning (MFRL) model applied to trust game data in R. The trust game is a widely used experimental paradigm in behavioral economics and psychology that measures trust and trustworthiness between individuals. In a typical trust game, one player (the investor) decides how much of an endowment to send to another player (the trustee). The amount sent is multiplied (usually tripled), and the trustee then decides how much to return to the investor.

The MFRL model attempts to capture how participants (N=183, role fo trustee over two 25 round games) learn from their experiences in the trust game using reinforcement learning principles combined with social preferences. The goal of our sensitivity analysis is to understand which parameters are identifiable from the data and which might need to be fixed at literature values.

We employ multiple complementary approaches to thoroughly assess parameter sensitivity:

* **Parameter Slice Analysis:**
    * We systematically vary each parameter individually while holding others constant at their optimal values.
    * This creates one-dimensional "slices" through the parameter space.
    * We calculate sensitivity as the average gradient of the negative log-likelihood (NLL) along each slice.
    * This approach quantifies how quickly model fit deteriorates as each parameter deviates from its optimal value.
* **Grid Search Analysis:**
    * We evaluate model fit across a multidimensional grid spanning the parameter space.
    * This approach reveals parameter interactions and regions of good fit.
    * It helps identify parameter trade-offs and potential identifiability issues when multiple parameter combinations yield similar model fits.
* **Fisher Information Matrix (FIM) Analysis:**
    * This theoretical approach calculates the curvature of the likelihood surface at the optimal parameter values.
    * The FIM provides:
        * Parameter precision estimates (diagonal elements).
        * Parameter interdependencies (off-diagonal elements).
        * Condition numbers that quantify overall identifiability.
    * While mathematically elegant, this approach can face numerical challenges with complex models.
* **Visualization and Statistical Summaries:**
    * We visualize sensitivity distributions across participants using boxplots and heatmaps to identify consistent patterns of parameter importance.
    * We also compute average NLL profiles to show how model fit changes as parameters vary from their optimal values.

Together, these approaches provide multiple perspectives on parameter identifiability, helping determine:

* Which parameters strongly affect model predictions.
* Which have flat likelihood surfaces and are difficult to identify.
* Where in parameter space sensitivity issues are most problematic.

This analysis examines how sensitive the model fit is to changes in each parameter across different participants. The analysis uses a reinforcement learning model with social preference components, specifically examining four key parameters:

1. **alpha**: Learning rate parameter that controls how quickly agents update their expectations based on new information. Higher values mean faster learning from recent outcomes.
2. **temp**: Temperature parameter for the softmax choice function, controlling choice stochasticity or exploration. Higher values indicate more random choices.
3. **envy**: Parameter related to disadvantageous inequality aversion (disutility from others getting more than oneself). Higher values indicate stronger aversion to receiving less than the partner.
4. **guilt**: Parameter related to advantageous inequality aversion (disutility from getting more than others). Higher values indicate stronger aversion to receiving more than the partner.

# 2. MFRL model

## Model-Free Reinforcement Learning (Q-Learning) with Fehr-Schmidt Utility

This section outlines the mathematical equations for a model-free reinforcement learning agent using Q-learning, incorporating the Fehr-Schmidt utility function to account for social preferences.

**1. Standard Q-Learning Update:**

The traditional Q-learning update rule is:

`Q(s, a) ← Q(s, a) + α [r + γ max_a' Q(s', a') - Q(s, a)]`

where:

* `Q(s, a)`: Q-value of taking action `a` in state `s`.
* `α`: Learning rate (0 < α ≤ 1).
* `r`: Immediate reward received after taking action `a` in state `s`.
* `γ`: Discount factor (0 ≤ γ < 1).
* `s'`: Next state.
* `max_a' Q(s', a')`: Maximum Q-value of any action in the next state `s'`.

**2. Fehr-Schmidt Utility Function:**

The Fehr-Schmidt utility function models inequity aversion. For two players, `i` and `j`, the utility of player `i` is:

`U_i(x_i, x_j) = x_i - α_i max{x_j - x_i, 0} - β_i max{x_i - x_j, 0}`

where:

* `x_i`: Payoff of player `i`.
* `x_j`: Payoff of player `j`.
* `α_i`: Envy parameter (measures aversion to being worse off than player `j`).
* `β_i`: Guilt parameter (measures aversion to being better off than player `j`).
* `α_i ≥ 0`, `β_i ≥ 0`, and `β_i ≤ α_i`.

**3. Modified Q-Learning Update with Fehr-Schmidt Utility:**

We modify the reward `r` in the Q-learning update to incorporate the Fehr-Schmidt utility. Assuming the agent's reward is `x_i` and the other player's is `x_j`, the modified reward `r'` is:

`r' = x_i - α_i max{x_j - x_i, 0} - β_i max{x_i - x_j, 0}`

The Q-learning update then becomes:

`Q(s, a) ← Q(s, a) + α [r' + γ max_a' Q(s', a') - Q(s, a)]`

or

`Q(s, a) ← Q(s, a) + α [x_i - α_i max{x_j - x_i, 0} - β_i max{x_i - x_j, 0} + γ max_a' Q(s', a') - Q(s, a)]`

**4. Softmax Action Selection with Temperature:**

To introduce exploration, we use a softmax action selection policy:

`P(a|s) = exp(Q(s, a) / τ) / Σ_a' exp(Q(s, a') / τ)`

where:

* `P(a|s)`: Probability of choosing action `a` in state `s`.
* `τ`: Temperature parameter (τ > 0). Higher temperature leads to more exploration.

**Summary of Parameters:**

* `α`: Learning rate.
* `γ`: Discount factor.
* `α_i`: Envy parameter.
* `β_i`: Guilt parameter.
* `τ`: Temperature parameter.

This section loads necessary R libraries for computation, parallel processing, numerical methods, and visualization. It also defines several key helper functions that are used in the MFRL model:

- **get_investment_bin**: Maps continuous investment values to discrete states (1-3) for the Q-learning model, allowing the model to categorize investment amounts into low, medium, and high ranges.
- **get_return_bin**: Maps continuous return proportions to discrete actions (1-6) for the Q-learning model, creating six equal-sized bins for the possible return proportions from 0 to 1.
- **calculate_fs_utility**: Implements the Fehr-Schmidt inequality aversion utility function, which adjusts the utility based on payoff differences between self and other, accounting for both disadvantageous inequality (envy) and advantageous inequality (guilt).

The model fitting algorithm operates as follows:

1.  **Input:** Accepts participant behavioral data and model parameter values.
2.  **Initialization:** Sets initial Q-values (expected utilities) for all state-action pairs.
3.  **Trial Loop:** Iterates through each trial:
    * Calculates action probabilities using the softmax function.
    * Updates Q-values based on temporal difference (TD) learning.
4.  **Likelihood Calculation:** Computes the negative log-likelihood (NLL) of the participant's observed choices.
5.  **Optional Output:** Returns internal model variables for diagnostic purposes, if requested.

This model captures both the learning process (via Q-learning) and social preferences (via the Fehr-Schmidt utility function) in trust game behavior.


```{r load_libraries}
# Core libraries
library(tidyverse)
library(parallel)
library(pbapply)  # For progress bars
library(numDeriv) # For numerical differentiation

# Visualization libraries
library(ggplot2)
library(viridis)
library(gridExtra)
library(reshape2)

# Set a seed for reproducibility
set.seed(123)
```

```{r helper_functions}
#' Convert investment value to a discrete state bin (1-3)
#' 
#' Maps continuous investment values to discrete states for the Q-learning model
#' 
#' @param investment Numeric investment amount
#' @return Integer state bin (1, 2, or 3)
get_investment_bin <- function(investment) {
  if (investment <= 7)   return(1)
  if (investment <= 14)  return(2)
  return(3)
}

#' Convert return proportion to a discrete action bin (1-6)
#' 
#' Maps continuous return proportions to discrete actions for the Q-learning model
#' 
#' @param return_prop Numeric return proportion (0-1)
#' @return Integer action bin (1-6)
get_return_bin <- function(return_prop) {
  if (return_prop < 0 || return_prop > 1) 
    stop("Return proportion must be between 0 and 1.")
  # 6 equal-sized bins in [0,1]
  return(min(floor(return_prop * 6) + 1, 6))
}

#' Calculate utility using Fehr-Schmidt inequality aversion model
#' 
#' Computes utility based on own payoff and other's payoff, accounting for
#' disadvantageous inequality (envy) and advantageous inequality (guilt)
#' 
#' @param own_payoff Numeric payoff for self
#' @param other_payoff Numeric payoff for other player
#' @param envy Parameter for disadvantageous inequality (when other gets more)
#' @param guilt Parameter for advantageous inequality (when self gets more)
#' @return Numeric utility value
calculate_fs_utility <- function(own_payoff, other_payoff, envy, guilt) {
  # Disadvantageous inequality (when other gets more than self)
  disadv <- max(other_payoff - own_payoff, 0)
  # Advantageous inequality (when self gets more than other)
  adv    <- max(own_payoff - other_payoff, 0)
  # Fehr-Schmidt utility
  return(own_payoff - envy * disadv - guilt * adv)
}

```

```{r direct_learning_model}
#' Model-free RL model for trustee choices in the Trust Game
#' 
#' This function implements a Q-learning model with Fehr-Schmidt social 
#' preferences. It computes the negative log likelihood of observed choices 
#' given the parameter values.
#' 
#' @param params_free Vector of free parameters to be estimated
#' @param data Dataframe containing observed game data
#' @param param_fixed List of fixed parameters (not to be estimated)
#' @param return_internals Whether to return internal model variables
#' @return Negative log likelihood (or model internals if requested)
direct_learning_model <- function(params_free, data, param_fixed = list(), return_internals = FALSE) {
  # 1) Reconstruct full parameter set from free and fixed parameters
  all_names <- c("alpha", "temp", "envy", "guilt")
  par_vals <- numeric(length(all_names))
  
  idx_free <- 1
  for (i in seq_along(all_names)) {
    nm <- all_names[i]
    if (!is.null(param_fixed[[nm]])) {
      # Use fixed parameter value
      par_vals[i] <- param_fixed[[nm]]
    } else {
      # Use value from free parameters vector
      if (idx_free <= length(params_free)) {
        par_vals[i] <- params_free[idx_free]
        idx_free <- idx_free + 1
      } else {
        warning(paste("Not enough free parameters provided. Missing value for", nm))
        par_vals[i] <- NA
      }
    }
  }
  
  # Input validation and bounds enforcement
  alpha <- min(max(par_vals[1], 0.001), 0.999)  # Avoid exact 0,1
  temp <- max(par_vals[2], 0.01)  # Prevent division by zero
  envy <- max(par_vals[3], 0)  # Non-negative
  guilt <- max(par_vals[4], 0)  # Non-negative
  
  # Model constants
  gamma <- 1  # Discount factor
  n_actions <- 6  # Number of discretized actions
  
  # Initialize variables
  n_trials <- nrow(data)
  neg_log_lik <- 0
  Q_values <- matrix(0, nrow=3, ncol=n_actions)
  
  # For storing model internals if requested
  if (return_internals) {
    internals <- list(
      trial = integer(),
      state = integer(),
      action = integer(),
      reward = numeric(),
      next_state = integer(),
      td_error = numeric(),
      probs = list(),
      log_lik = numeric(),
      Q_values = list()
    )
  }
  
  # Track the current game (to reset Q-values between games)
  current_game <- data$gameNum.f[1]
  
  # Loop through all trials
  for (t in seq_len(n_trials)) {
    invest_amt <- data$investment[t]
    actual_return <- data$return[t]
    
    # Skip invalid trials
    if (is.na(invest_amt) || is.na(actual_return) || invest_amt == 0) 
      next
    
    # Reset Q-values at the start of a new game
    if (t > 1 && data$gameNum.f[t] != current_game) {
      Q_values <- matrix(0, nrow=3, ncol=6)
      current_game <- data$gameNum.f[t]
    }
    
    # Get current state (based on investment amount)
    s_bin <- get_investment_bin(invest_amt)
    
    # Calculate action probabilities using softmax
    Q_s <- Q_values[s_bin,]
    shift <- Q_s - max(Q_s)  # For numerical stability
    probs <- exp(shift / temp)
    probs <- pmax(probs, 1e-12)  # Avoid zero probabilities
    probs <- probs / sum(probs)
    
    # Get actual chosen action (from observed return)
    actual_prop <- actual_return / (3 * invest_amt)
    chosen_bin <- get_return_bin(actual_prop)
    
    # Ensure action bin is valid
    if (chosen_bin < 1 || chosen_bin > 6) {
      warning(paste("Invalid action bin:", chosen_bin, "for return proportion:", actual_prop))
      chosen_bin <- min(max(chosen_bin, 1), 6)
    }
    
    # Update negative log likelihood
    current_ll <- log(probs[chosen_bin])
    neg_log_lik <- neg_log_lik - current_ll
    
    # Calculate payoffs and reward
    trustee_payoff <- 3 * invest_amt - actual_return
    investor_payoff <- actual_return - invest_amt
    reward <- calculate_fs_utility(trustee_payoff, investor_payoff, envy, guilt)
    
    # Calculate future value for TD learning
    if (t < n_trials) {
      next_invest <- data$investment[t + 1]
      if (!is.na(next_invest) && next_invest > 0) {
        next_s_bin <- get_investment_bin(next_invest)
        future_val <- max(Q_values[next_s_bin,])
      } else {
        future_val <- 0
      }
    } else {
      future_val <- 0
    }
    
    # TD learning update
    pe <- reward + gamma*future_val - Q_values[s_bin, chosen_bin]
    Q_values[s_bin, chosen_bin] <- Q_values[s_bin, chosen_bin] + alpha*pe
    
    # Store internals if requested
    if (return_internals) {
      internals$trial <- c(internals$trial, t)
      internals$state <- c(internals$state, s_bin)
      internals$action <- c(internals$action, chosen_bin)
      internals$reward <- c(internals$reward, reward)
      internals$next_state <- c(internals$next_state, ifelse(t < n_trials, next_s_bin, NA))
      internals$td_error <- c(internals$td_error, pe)
      internals$probs[[length(internals$probs) + 1]] <- probs
      internals$log_lik <- c(internals$log_lik, current_ll)
      internals$Q_values[[length(internals$Q_values) + 1]] <- Q_values
    }
  }
  
  # Return results
  if (return_internals) {
    return(list(neg_log_lik = neg_log_lik, internals = internals, final_Q = Q_values))
  } else {
    return(neg_log_lik)
  }
}
```

```{r}
# fit MFRL to dataset and get parameters 
```


# 3. Grid-Based Parameter Sensitivity Analysis Function

The function `calculate_grid_sensitivity` implements a comprehensive exploration of model behavior across the parameter space. It works by:

1. Creating a grid of parameter combinations by taking all possible combinations of specified parameter values
2. Evaluating the model's negative log-likelihood (NLL) for each parameter combination
3. Identifying which combinations produce the best and worst model fits

This approach is particularly valuable for detecting complex interactions between parameters, identifying regions of parameter space where the model performs well or poorly
and revealing parameter correlations that might affect identifiability

The function supports parallel processing to handle the computational demands of evaluating many parameter combinations efficiently. For each evaluated point in parameter space, it captures both the NLL value and any potential errors, ensuring robustness even when some parameter combinations might cause numerical issues.

The output includes the full results data frame, the minimum NLL found, and the corresponding best parameter set, providing a comprehensive view of how the parameter space affects model fit.


```{r grid_sensitivity_function}
#' Calculate sensitivity across a grid of parameter values
#' 
#' @param model_fn Function that calculates negative log-likelihood
#' @param data Dataframe with participant data
#' @param param_grid List of parameter values to evaluate
#' @param n_cores Number of cores for parallel processing
#' @return List with sensitivity analysis results
calculate_grid_sensitivity <- function(model_fn, data, param_grid, n_cores = 1) {
  # Ensure param_grid is properly formatted
  if (!is.list(param_grid)) {
    stop("param_grid must be a list with parameter names as keys and vectors of values")
  }
  
  # Get parameter names and create all combinations
  param_names <- names(param_grid)
  param_combinations <- expand.grid(param_grid)
  
  # Function to evaluate NLL at a specific parameter combination
  evaluate_params <- function(param_row) {
    # Convert row to named list
    params <- as.list(param_row)
    names(params) <- param_names
    
    # Calculate NLL
    tryCatch({
      nll <- model_fn(unlist(params), data)
      return(list(params = params, nll = nll, error = FALSE))
    }, error = function(e) {
      return(list(params = params, nll = NA, error = TRUE, 
                 message = as.character(e)))
    })
  }
  
  # Parallel processing if n_cores > 1
  if (n_cores > 1) {
    # Set up parallel cluster
    cl <- makeCluster(n_cores)
    on.exit(stopCluster(cl))
    
    # Export necessary functions and data
    clusterExport(cl, c("model_fn", "data", "param_names","get_investment_bin",  "get_return_bin",  "calculate_fs_utility"), 
                 envir = environment())
    
    # Evaluate all parameter combinations in parallel
    if (requireNamespace("pbapply", quietly = TRUE)) {
      results <- pblapply(1:nrow(param_combinations), function(i) {
        evaluate_params(param_combinations[i, ])
      }, cl = cl)
    } else {
      results <- parLapply(cl, 1:nrow(param_combinations), function(i) {
        evaluate_params(param_combinations[i, ])
      })
    }
  } else {
    # Sequential processing
    results <- lapply(1:nrow(param_combinations), function(i) {
      evaluate_params(param_combinations[i, ])
    })
  }
  
  # Extract NLL values and reshape for analysis
  nll_matrix <- matrix(NA, nrow = nrow(param_combinations), ncol = 1)
  colnames(nll_matrix) <- "nll"
  
  for (i in 1:length(results)) {
    nll_matrix[i, ] <- results[[i]]$nll
  }
  
  # Combine with parameter values
  results_df <- cbind(param_combinations, nll_matrix)
  
  # Create output with additional analysis
  output <- list(
    param_grid = param_grid,
    results_df = results_df,
    min_nll = min(results_df$nll, na.rm = TRUE),
    best_params = results_df[which.min(results_df$nll), ]
  )
  
  return(output)
}
```

# 4. Parameter Slice Analysis Function

The function `calculate_parameter_slices` takes a more targeted approach to sensitivity analysis by:

1. Holding all parameters constant at their "center" (typically the best-fitting) values
2. Varying one parameter at a time across a specified range
3. Computing the NLL at each point to create "slices" through the likelihood surface

This one-at-a-time approach allows for precise quantification of each parameter's individual contribution to model fit while controlling for the effects of other parameters. The function generates parameter ranges either from global bounds or by creating reasonable ranges around the center values, adapting to the characteristics of each parameter (e.g., learning rates constrained between 0 and 1).

For each parameter slice, the function calculates:
- The raw NLL values
- The "delta NLL" (difference from minimum NLL in the slice)
- A sensitivity metric based on the average absolute gradient of the delta NLL curve

This gradient-based sensitivity metric quantifies how quickly the model fit deteriorates as each parameter deviates from its optimal value. Higher sensitivity values indicate parameters that strongly influence model fit and are therefore more reliably identifiable from the data.

```{r parameter_slices_function}
#' Calculate NLL slices for each parameter
#' 
#' @param model_fn Function that calculates negative log-likelihood
#' @param data Dataframe with participant data
#' @param center_params Central parameter values to slice through
#' @param param_ranges List of parameter ranges for slices
#' @param n_points Number of points per parameter
#' @param n_cores Number of cores for parallel processing
#' @return List with slice analysis results
calculate_parameter_slices <- function(model_fn, data, center_params, 
                                      param_ranges = NULL, global_param_ranges = NULL,
                                      n_points = 20, n_cores = 1) {
  # Ensure center_params is a list
  if (!is.list(center_params)) {
    center_params <- as.list(center_params)
  }
  
  # Get parameter names
  param_names <- names(center_params)
  
  # Create default parameter ranges if not provided
  if (is.null(param_ranges)) {
    param_ranges <- list()
    for (param in param_names) {
      # Check if we have a global range for this parameter
      if (!is.null(global_param_ranges) && param %in% names(global_param_ranges)) {
        # Use global range
        param_min <- global_param_ranges[[param]][1]
        param_max <- global_param_ranges[[param]][2]
        param_ranges[[param]] <- seq(param_min, param_max, length.out = n_points)
      } else {
        # Use default range centered around current value
        current_value <- center_params[[param]]
        
        # Set ranges based on parameter type
        if (param == "alpha") {
          # Learning rate between 0 and 1
          param_ranges[[param]] <- seq(max(0.01, current_value/2), 
                                     min(0.99, current_value*2), 
                                     length.out = n_points)
        } else if (param == "temp") {
          # Temperature (positive)
          param_ranges[[param]] <- seq(max(0.1, current_value/2), 
                                     current_value*2, 
                                     length.out = n_points)
        } else if (param %in% c("envy", "guilt")) {
          # Social preference parameters (non-negative)
          param_ranges[[param]] <- seq(max(0, current_value/2), 
                                     current_value*2, 
                                     length.out = n_points)
        } else {
          # Default range
          param_ranges[[param]] <- seq(max(0.01, current_value/2), 
                                     current_value*2, 
                                     length.out = n_points)
        }
      }
    }
  }

  
  # Function to calculate NLL for a single parameter slice
  calculate_slice <- function(param) {
    param_values <- param_ranges[[param]]
    nll_values <- numeric(length(param_values))
    
    # Calculate NLL for each parameter value
    for (i in seq_along(param_values)) {
      # Make a copy of center parameters
      modified_params <- center_params
      
      # Change only the parameter of interest
      modified_params[[param]] <- param_values[i]
      
      # Calculate NLL
      nll_values[i] <- tryCatch({
        model_fn(unlist(modified_params), data)
      }, error = function(e) {
        return(NA)
      })
    }
    
    # Return results for this parameter
    df <- data.frame(
      parameter = param,
      value = param_values,
      nll = nll_values,
      delta_nll = nll_values - min(nll_values, na.rm = TRUE)
    )
    
    return(df)
  }
  
  # Calculate slices for all parameters, with parallel processing if needed
  if (n_cores > 1) {
    # Set up parallel cluster
    cl <- makeCluster(n_cores)
    on.exit(stopCluster(cl))
    
    # Export necessary functions and data
    clusterExport(cl, c("model_fn", "data", "center_params", "param_ranges", 
                       "n_points","get_investment_bin",  "get_return_bin",  "calculate_fs_utility"), envir = environment())
    
    # Calculate slices in parallel
    if (requireNamespace("pbapply", quietly = TRUE)) {
      slice_results <- pblapply(param_names, calculate_slice, cl = cl)
    } else {
      slice_results <- parLapply(cl, param_names, calculate_slice)
    }
  } else {
    # Sequential processing
    slice_results <- lapply(param_names, calculate_slice)
  }
  
  # Name the results
  names(slice_results) <- param_names
  
  # Calculate parameter sensitivity (slope of NLL)
  sensitivity <- lapply(slice_results, function(df) {
    # Calculate the average slope of the delta_nll curve
    x <- df$value
    y <- df$delta_nll
    
    # Normalize x to [0,1] for fair comparison across parameters
    x_norm <- (x - min(x)) / (max(x) - min(x))
    
    # Calculate average absolute gradient
    diffs <- diff(y) / diff(x_norm)
    mean_gradient <- mean(abs(diffs), na.rm = TRUE)
    
    return(mean_gradient)
  })
  
  # Return results
  return(list(
    slices = slice_results,
    center_params = center_params,
    param_ranges = param_ranges,
    sensitivity = sensitivity,
    combined_data = do.call(rbind, slice_results)
  ))
}
```

# 5. FIM-Based Parameter Sensitivity Analysis


The function `calculate_fisher_information` implements a more mathematically rigorous approach to parameter identifiability using the Fisher Information Matrix (FIM). The FIM provides a formal measure of how much information the data contains about each parameter and how parameters might be correlated.

The function works by:

1. Computing the Hessian matrix (second derivatives) of the negative log-likelihood with respect to all parameters
2. Taking the negative of this Hessian to form the Fisher Information Matrix
3. Extracting various diagnostics from this matrix

Key outputs include:

- **Parameter precision** (diagonal elements): Higher values indicate more precise parameter estimates
- **Condition number**: The ratio of largest to smallest eigenvalues, with high values (>100) indicating potential identifiability issues
- **Standardized FIM**: A correlation-like matrix showing parameter interdependencies

The function uses numerical differentiation methods from the numDeriv package for stability, with a fallback to manual second derivative calculation if needed. It also handles potential numerical issues like non-positive definite matrices, making it robust for real-world data analysis.

This FIM-based approach complements the other sensitivity methods by providing a theoretical foundation for parameter identifiability based on information theory.


```{r fim_function}
#' Calculate Fisher Information Matrix for parameter identifiability
#' 
#' @param model_fn Function that calculates negative log-likelihood
#' @param params Parameter vector
#' @param data Data for model evaluation
#' @param epsilon Step size for numerical derivatives
#' @return FIM with parameter identifiability metrics
calculate_fisher_information <- function(model_fn, params, data, epsilon = 0.01) {
  # Ensure params is a named vector
  if (is.list(params)) {
    params <- unlist(params)
  }
  
  # Get parameter names and number
  param_names <- names(params)
  n_params <- length(params)
  
  # Calculate Hessian matrix of NLL (second derivatives)
  # Use numDeriv package for stable numerical differentiation
  if (requireNamespace("numDeriv", quietly = TRUE)) {
    hessian <- tryCatch({
      # Note: model_fn returns negative log-likelihood, so hessian is correct sign
      numDeriv::hessian(function(p) model_fn(p, data), params)
    }, error = function(e) {
      # If hessian fails, use more robust but slower method
      warning("Standard Hessian calculation failed, using manual calculation")
      
      # Initialize matrix
      H <- matrix(0, n_params, n_params)
      
      # Manual second derivative calculation
      for (i in 1:n_params) {
        for (j in 1:n_params) {
          # Parameter step sizes
          ei <- epsilon * max(1e-2, abs(params[i]))
          ej <- epsilon * max(1e-2, abs(params[j]))
          
          # Create modified parameter vectors
          params_pp <- params_pm <- params_mp <- params_mm <- params
          
          params_pp[i] <- params[i] + ei; params_pp[j] <- params[j] + ej
          params_pm[i] <- params[i] + ei; params_pm[j] <- params[j] - ej
          params_mp[i] <- params[i] - ei; params_mp[j] <- params[j] + ej
          params_mm[i] <- params[i] - ei; params_mm[j] <- params[j] - ej
          
          # Calculate function values
          f_pp <- model_fn(params_pp, data)
          f_pm <- model_fn(params_pm, data)
          f_mp <- model_fn(params_mp, data)
          f_mm <- model_fn(params_mm, data)
          
          # Central difference formula for mixed second derivative
          H[i, j] <- (f_pp - f_pm - f_mp + f_mm) / (4 * ei * ej)
        }
      }
      
      return(H)
    })
  } else {
    stop("numDeriv package is required for FIM calculation")
  }
  
  # Fisher Information Matrix is the expected value of the Hessian
  # For point estimates, we use the negative Hessian at the MAP
  fim <- -hessian
  
  # Name the rows and columns
  rownames(fim) <- param_names
  colnames(fim) <- param_names
  
  # Extract diagonal elements (parameter precision)
  parameter_precision <- diag(fim)
  
  # Calculate condition number (ratio of max to min eigenvalue)
  # High condition number indicates poor identifiability
  eigvals <- tryCatch({
    eigen(fim, symmetric = TRUE, only.values = TRUE)$values
  }, error = function(e) {
    rep(NA, n_params)
  })
  
  # Handle non-positive definite matrices
  positive_eigvals <- eigvals[eigvals > 1e-10]
  if (length(positive_eigvals) > 0) {
    condition_number <- max(positive_eigvals) / min(positive_eigvals)
  } else {
    condition_number <- NA
  }
  
  # Standardize FIM to correlation-like matrix
  # This shows parameter correlations and relative identifiability
  fim_std <- matrix(0, n_params, n_params)
  for (i in 1:n_params) {
    for (j in 1:n_params) {
      if (parameter_precision[i] > 0 && parameter_precision[j] > 0) {
        fim_std[i, j] <- fim[i, j] / sqrt(parameter_precision[i] * parameter_precision[j])
      } else {
        fim_std[i, j] <- NA
      }
    }
  }
  
  # Return complete results
  return(list(
    fim = fim,
    fim_standardized = fim_std,
    parameter_precision = parameter_precision,
    condition_number = condition_number,
    eigenvalues = eigvals,
    param_names = param_names
  ))
}
```

# 6. Comprehensive Participant Sensitivity Analysis

The function `analyze_participant_sensitivity` performs a multi-method sensitivity analysis for a single participant's data. It integrates the three previous approaches (FIM, parameter slices, and grid search) to provide a comprehensive view of parameter identifiability for that individual.

The function begins with diagnostic tests to ensure the model works with the provided parameters, reporting any issues encountered. It then sequentially applies:

1. Fisher Information Matrix calculation at the center parameters
2. Parameter slice analysis across specified ranges
3. Optional grid-based sensitivity analysis if a parameter grid is provided

This multi-pronged approach is valuable because each method has different strengths:

- FIM provides theoretical identifiability and parameter correlations
- Parameter slices give intuitive sensitivity measures for each parameter
- Grid search reveals complex interactions between parameters

The function handles errors gracefully at each stage, allowing the analysis to continue even if one method fails. It returns a comprehensive results structure that includes all successful analyses along with participant metadata, enabling both detailed individual assessment and integration into population-level analyses.


```{r comprehensive_analysis}
#' Analyze parameter sensitivity for a single participant
#' 
#' @param model_fn Function that calculates negative log-likelihood
#' @param data Participant data
#' @param center_params Parameter values to center the analysis on
#' @param param_grid List of parameter values for grid analysis (optional)
#' @return List with sensitivity analysis results
analyze_participant_sensitivity <- function(model_fn, data, center_params, 
                                          param_grid = NULL,global_param_ranges = NULL) {
  # Print some diagnostic info
  cat("Analyzing participant:", unique(data$playerId)[1], "\n")
  cat("Parameters:", paste(names(center_params), "=", unlist(center_params), collapse=", "), "\n")
  
  # Test if model works with parameters
  test_nll <- tryCatch({
    nll <- model_fn(unlist(center_params), data)
    cat("Model test successful: NLL =", nll, "\n")
    nll
  }, error = function(e) {
    cat("ERROR: Model function failed:", e$message, "\n")
    return(NA)
  })
  
  # If model doesn't work with these parameters, return error
  if (is.na(test_nll)) {
    return(list(
      error = TRUE,
      message = "Model function failed with these parameters",
      center_params = center_params,
      participant_id = unique(data$playerId)[1]
    ))
  }
  
  # Initialize results
  fim_results <- NULL
  slice_results <- NULL
  grid_results <- NULL
  
  # 1. Calculate FIM at center_params
  fim_results <- tryCatch({
    calculate_fisher_information(model_fn, center_params, data)
  }, error = function(e) {
    cat("FIM calculation failed:", e$message, "\n")
    return(NULL)
  })
  
  # 2. Calculate parameter slices
  slice_results <- tryCatch({
    calculate_parameter_slices(model_fn, data, center_params, 
                             global_param_ranges = global_param_ranges)  # Add this parameter
  }, error = function(e) {
    cat("Parameter slice calculation failed:", e$message, "\n")
    return(NULL)
  })
  
  # 3. Calculate grid sensitivity if param_grid is provided
  if (!is.null(param_grid)) {
    grid_results <- tryCatch({
      calculate_grid_sensitivity(model_fn, data, param_grid)
    }, error = function(e) {
      cat("Grid sensitivity calculation failed:", e$message, "\n")
      return(NULL)
    })
  }
  
  # Return comprehensive results
  list(
    fim = fim_results,
    slices = slice_results,
    grid = grid_results,
    center_params = center_params,
    participant_data_summary = list(
      n_trials = nrow(data),
      playerId = unique(data$playerId)[1]
    )
  )
}
```

# 7. Parallelized Multi-Participant Analysis

The function `analyze_population_sensitivity` extends the comprehensive sensitivity analysis to multiple participants, leveraging parallel processing for efficiency. This is crucial for studies with many participants, as sensitivity analyses are computationally intensive.

<!-- The function: -->
<!-- 1. Matches participant data with their corresponding fitted parameters -->
<!-- 2. Defines a worker function to process each participant independently -->
<!-- 3. Distributes these analyses across multiple cores if requested -->
<!-- 4. Collects and organizes the results from all participants -->

<!-- The implementation is flexible, accepting either a data frame with a participant ID column or a pre-split list of participant datasets. It handles the complexities of exporting necessary functions and data to the parallel workers, and manages the proper shutdown of the parallel cluster when analysis is complete. -->

<!-- The function also includes progress tracking through the pbapply package, providing visual feedback during long-running analyses. After collecting individual results, it calls `summarize_population_sensitivity` to generate population-level summaries, creating a hierarchical results structure that preserves both individual-level details and population-level patterns. -->

```{r multi_participant_analysis}
#' Analyze sensitivity across multiple participants in parallel
#' 
#' @param model_fn Function that calculates negative log-likelihood
#' @param data_list List of participant datasets or data frame with playerId column
#' @param params_df Dataframe with fitted parameters (must have playerId column)
#' @param param_grid List of parameter values for grid analysis (optional)
#' @param n_cores Number of cores for parallel processing
#' @return List with sensitivity analysis results for all participants
analyze_population_sensitivity <- function(model_fn, data_list, params_df, 
                                         param_grid = NULL, global_param_ranges = NULL,n_cores = 1) {
  # Check if data_list is a data frame with playerId
  if (is.data.frame(data_list) && "playerId" %in% names(data_list)) {
    # Split data by playerId
    participant_data <- split(data_list, data_list$playerId)
  } else if (is.list(data_list)) {
    participant_data <- data_list
  } else {
    stop("data_list must be either a list of participant datasets or a data frame with playerId column")
  }
  
  # Ensure params_df has playerId column
  if (!("playerId" %in% names(params_df))) {
    stop("params_df must have a playerId column")
  }
  
  # Get participant IDs present in both datasets
  participant_ids <- intersect(
    names(participant_data), 
    params_df$playerId
  )
  
  if (length(participant_ids) == 0) {
    stop("No matching participant IDs found between data and parameters")
  }
  
  # Function to process a single participant
  process_participant <- function(p_id) {
    # Get participant data
    p_data <- participant_data[[p_id]]
    
    # Get participant parameters
    p_params <- params_df[params_df$playerId == p_id, ]
    
    # Skip if no parameters found
    if (nrow(p_params) == 0) {
      return(NULL)
    }
    
    # Extract parameter values
    center_params <- list(
      alpha = p_params$alpha,
      temp = p_params$temp,
      envy = p_params$envy,
      guilt = p_params$guilt
    )
    
    # Remove any NA parameters
    center_params <- center_params[!is.na(unlist(center_params))]
    
    # Run analysis
    tryCatch({
      analyze_participant_sensitivity(model_fn, p_data, center_params, param_grid, global_param_ranges = global_param_ranges)
    }, error = function(e) {
      # Return error information
      list(
        error = TRUE,
        message = as.character(e),
        participant_id = p_id
      )
    })
  }
  
  # Process all participants, with parallel processing if requested
  if (n_cores > 1) {
    # Set up parallel cluster
    cl <- makeCluster(n_cores)
    on.exit(stopCluster(cl))
    
    # Export necessary functions and data
    clusterExport(cl, c("model_fn", "participant_data", "params_df", 
                       "param_grid", "analyze_participant_sensitivity",
                       "calculate_fisher_information", 
                       "calculate_parameter_slices",
                       "calculate_grid_sensitivity","get_investment_bin",  "get_return_bin",  "calculate_fs_utility"), 
                 envir = environment())
    
    # Process participants in parallel
    if (requireNamespace("pbapply", quietly = TRUE)) {
      results <- pblapply(participant_ids, process_participant, cl = cl)
    } else {
      results <- parLapply(cl, participant_ids, process_participant)
    }
  } else {
    # Sequential processing
    results <- lapply(participant_ids, process_participant)
  }
  
  # Name results with participant IDs
  names(results) <- participant_ids
  
  # Summarize results across participants
  summary <- summarize_population_sensitivity(results)
  
  # Return complete results
  list(
    participant_results = results,
    summary = summary,
    participant_ids = participant_ids
  )
}
```

# 8. Population-Level Summary Function

The function `summarize_population_sensitivity` aggregates sensitivity results across participants to identify general patterns of parameter identifiability. This population-level view is essential for drawing broader conclusions about which parameters are consistently identifiable versus those that vary in importance across individuals.

<!-- The function performs several key aggregation steps: -->
<!-- 1. Filtering out null or error results to focus on valid analyses -->
<!-- 2. Collecting FIM diagonal values (parameter precision) across participants -->
<!-- 3. Gathering sensitivity metrics from parameter slices across participants -->
<!-- 4. Creating a sensitivity matrix with rows for participants and columns for parameters -->
<!-- 5. Computing summary statistics (mean, median, standard deviation, min, max) for each parameter -->

<!-- The sensitivity matrix is particularly valuable as it enables both visualization of parameter importance across the population and identification of outlier participants who show unusual sensitivity patterns. The function handles various edge cases, such as missing parameters or different parameter sets across participants, ensuring robust aggregation even with heterogeneous data. -->

<!-- The comprehensive summary statistics provide quantitative measures of each parameter's importance and consistency across the population, informing decisions about which parameters to estimate versus fix in future modeling efforts. -->


```{r summary_function}
#' Summarize sensitivity analysis results across participants
#' 
#' @param participant_results List of participant sensitivity results
#' @return List with aggregated sensitivity metrics
#' Summarize sensitivity analysis results across participants
#' 
#' @param participant_results List of participant sensitivity results
#' @return List with aggregated sensitivity metrics
summarize_population_sensitivity <- function(participant_results) {
  # Remove NULL or error results
  valid_results <- participant_results[!sapply(participant_results, is.null)]
  valid_results <- valid_results[!sapply(valid_results, function(x) {
    isTRUE(x$error)
  })]
  
  # Get number of valid results
  n_valid <- length(valid_results)
  if (n_valid == 0) {
    warning("No valid participant results found for summary")
    return(list(
      n_participants = 0,
      error = "No valid participant results"
    ))
  }
  
  # Parameter names to summarize
  param_names <- unique(unlist(lapply(valid_results, function(x) {
    if (!is.null(x$center_params)) {
      return(names(x$center_params))
    } else {
      return(NULL)
    }
  })))
  
  if (length(param_names) == 0) {
    warning("No valid parameter names found")
    return(list(
      n_participants = n_valid,
      error = "No valid parameter names found"
    ))
  }
  
  # Calculate average FIM diagonal values (parameter precision)
  fim_diagonals <- lapply(valid_results, function(x) {
    if (is.null(x$fim) || is.null(x$fim$parameter_precision)) {
      return(NULL)
    }
    
    precision <- x$fim$parameter_precision
    names(precision) <- x$fim$param_names
    return(precision)
  })
  
  # Remove NULL values and combine
  fim_diagonals <- fim_diagonals[!sapply(fim_diagonals, is.null)]
  
  # Check if we have any valid FIM results
  fim_summary <- list()
  if (length(fim_diagonals) > 0) {
    fim_diagonal_df <- as.data.frame(do.call(rbind, fim_diagonals))
    
    # Calculate summary statistics for FIM diagonals
    fim_summary <- lapply(param_names, function(param) {
      if (!(param %in% names(fim_diagonal_df))) {
        return(list(mean = NA, median = NA, sd = NA))
      }
      
      values <- fim_diagonal_df[[param]]
      list(
        mean = mean(values, na.rm = TRUE),
        median = median(values, na.rm = TRUE),
        sd = sd(values, na.rm = TRUE),
        min = min(values, na.rm = TRUE),
        max = max(values, na.rm = TRUE)
      )
    })
    names(fim_summary) <- param_names
  } else {
    fim_diagonal_df <- data.frame()
  }
  
  # Collect parameter slice sensitivities
  slice_sensitivities <- lapply(valid_results, function(x) {
    if (is.null(x$slices) || is.null(x$slices$sensitivity)) {
      return(NULL)
    }
    
    sens <- x$slices$sensitivity
    return(sens)
  })
  
  # Remove NULL values
  slice_sensitivities <- slice_sensitivities[!sapply(slice_sensitivities, is.null)]
  
  # Check if we have any valid slice sensitivities
  slice_summary <- list()
  sensitivity_matrix <- matrix(0, 0, 0)  # Initialize with empty matrix
  
  if (length(slice_sensitivities) > 0) {
    # Create a matrix with rows for participants and columns for parameters
    sensitivity_matrix <- matrix(NA, nrow = length(slice_sensitivities), 
                               ncol = length(param_names))
    colnames(sensitivity_matrix) <- param_names
    
    for (i in seq_along(slice_sensitivities)) {
      sens <- slice_sensitivities[[i]]
      for (param in names(sens)) {
        if (param %in% param_names) {
          col_idx <- which(param_names == param)
          sensitivity_matrix[i, col_idx] <- sens[[param]]
        }
      }
    }
    
    # Calculate summary statistics for slice sensitivities
    # BUGFIX: Check if sensitivity_matrix has any columns
    if (ncol(sensitivity_matrix) > 0) {
      slice_summary <- lapply(1:ncol(sensitivity_matrix), function(j) {
        values <- sensitivity_matrix[, j]
        list(
          mean = mean(values, na.rm = TRUE),
          median = median(values, na.rm = TRUE),
          sd = sd(values, na.rm = TRUE),
          min = min(values, na.rm = TRUE),
          max = max(values, na.rm = TRUE)
        )
      })
      names(slice_summary) <- colnames(sensitivity_matrix)
    }
  }
  
  # Return all summaries
  list(
    n_participants = n_valid,
    fim_diagonal_summary = fim_summary,
    slice_sensitivity_summary = slice_summary,
    fim_diagonal_data = if(exists("fim_diagonal_df")) fim_diagonal_df else data.frame(),
    sensitivity_matrix = sensitivity_matrix,
    param_names = param_names
  )
}
```

# 9. Visualization Functions

This section implements three key visualization functions that translate the technical sensitivity metrics into intuitive graphical representations:

## Plot_sensitivity_boxplots

Creates boxplots showing the distribution of sensitivity values for each parameter across participants. Higher Median Sensitivity: A parameter with a higher median sensitivity is more influential in determining the model’s NLL. It means that small deviations from the optimal value lead to large increases in NLL. Low Sensitivity: If the boxplot shows low sensitivity values (with little variability), it suggests that changes in that parameter have minimal impact on the NLL. Such parameters may be candidates for fixing at a literature value rather than estimating from the data. The implementation includes:
    - Diagnostic checks of the sensitivity matrix
    - Manual creation of a plotting data frame
    - Application of a logarithmic scale to handle the wide range of sensitivity values
    - Addition of individual data points to show the full distribution

## plot_sensitivity_heatmap

Generates a heatmap visualization of parameter sensitivity across all participants, with:
    - Optional normalization of sensitivity values by parameter
    - Safety checks to handle missing or invalid data
    - Color coding using the viridis palette for accessibility
    - Proper labeling and theme customization

## plot_average_nll_profiles

Creates line plots showing how the NLL changes as each parameter deviates from its optimal value, featuring:
    - Extensive diagnostic checks to identify and report issues
    - Combination of slice data across participants
    - Normalization of parameter values to enable cross-parameter comparison
    - Calculation of mean and standard error for delta NLL values
    - Ribbon plots showing uncertainty around the mean profiles

## plot_grid_results

function visualizes the results of grid searches, creating heatmaps for each parameter pair to show how combinations of parameters affect model fit.




```{r visualization_functions}

#' Create boxplots of parameter sensitivity across participants
#' 
#' @param summary Results from summarize_population_sensitivity
#' @return ggplot object with sensitivity boxplots
plot_sensitivity_boxplots <- function(summary) {
  # First, print diagnostic info
  cat("Sensitivity matrix information:\n")
  cat("Is NULL?", is.null(summary$sensitivity_matrix), "\n")
  cat("Is matrix?", is.matrix(summary$sensitivity_matrix), "\n")
  if (!is.null(summary$sensitivity_matrix) && is.matrix(summary$sensitivity_matrix)) {
    cat("Dimensions:", dim(summary$sensitivity_matrix), "\n")
    cat("Column names:", paste(colnames(summary$sensitivity_matrix), collapse=", "), "\n")
  }
  
  # Check for an empty or invalid matrix
  if (is.null(summary$sensitivity_matrix) || 
      !is.matrix(summary$sensitivity_matrix) ||
      nrow(summary$sensitivity_matrix) == 0 || 
      ncol(summary$sensitivity_matrix) == 0) {
    return(ggplot() + 
             annotate("text", x = 0.5, y = 0.5, 
                     label = "No valid sensitivity data available") +
             theme_void())
  }
  
  # Create a manual data frame instead of using melt
  plot_data <- data.frame()
  
  # Process each parameter manually
  for (j in 1:ncol(summary$sensitivity_matrix)) {
    if (j > length(colnames(summary$sensitivity_matrix))) {
      # Skip if column name doesn't exist
      next
    }
    
    param_name <- colnames(summary$sensitivity_matrix)[j]
    values <- summary$sensitivity_matrix[, j]
    
    # Create a data frame for this parameter
    param_data <- data.frame(
      parameter = rep(param_name, length(values)),
      sensitivity = values,
      participant = 1:length(values)
    )
    
    # Combine with main data frame
    plot_data <- rbind(plot_data, param_data)
  }
  
  # Remove NA values
  plot_data <- plot_data[!is.na(plot_data$sensitivity), ]
  
  # Check if we have any data
  if (nrow(plot_data) == 0) {
    return(ggplot() + 
             annotate("text", x = 0.5, y = 0.5, 
                     label = "No valid sensitivity data after processing") +
             theme_void())
  }
  
  # Create the plot
  # ggplot(plot_data, aes(x = parameter, y = sensitivity)) +
  #   geom_boxplot(fill = "lightblue",outlier.shape = NA) +
  #   geom_jitter(width = 0.2, alpha = 0.5) + 
  #   theme_minimal() +
  #   labs(title = "Parameter Sensitivity Distribution",
  #        subtitle = paste("Based on", summary$n_participants, "participants"),
  #        x = "Parameter",
  #        y = "Sensitivity (Higher = More Identifiable)") +
  #   theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  ggplot(plot_data, aes(x = parameter, y = sensitivity)) +
    geom_boxplot(fill = "lightblue", alpha = 0.6) +
    geom_jitter(width = 0.2, alpha = 0.5, color = "darkblue") +
    scale_y_continuous(trans = 'log10') +  # Uncomment or remove based on your data
    theme_minimal() +
    labs(title = "Parameter Sensitivity Distribution",
         subtitle = paste("Based on", summary$n_participants, "participants"),
         x = "Parameter",
         y = "Sensitivity in Log Scale (Higher = More Identifiable)")
}

########################
#' @param summary Results from summarize_population_sensitivity
#' @param normalize Whether to normalize sensitivity values
#' @return ggplot object with sensitivity heatmap
#' Create heatmap of parameter sensitivity across participants
plot_sensitivity_heatmap <- function(summary, normalize = TRUE) {
  # Check if sensitivity matrix exists and has valid data
  if (is.null(summary$sensitivity_matrix) || 
      !is.matrix(summary$sensitivity_matrix) ||
      length(summary$sensitivity_matrix) == 0 || 
      nrow(summary$sensitivity_matrix) == 0 || 
      ncol(summary$sensitivity_matrix) == 0) {
    
    # Return an empty plot with a message
    return(ggplot() + 
             annotate("text", x = 0.5, y = 0.5, 
                     label = "No valid sensitivity data available") +
             theme_void())
  }
  
  # Try to process and plot
  tryCatch({
    # Get sensitivity matrix and convert to data frame first
    sens_matrix <- summary$sensitivity_matrix
    sens_df <- as.data.frame(sens_matrix)
    sens_df$participant <- 1:nrow(sens_df)
    
    # Optionally normalize by column (do this before converting to long format)
    if (normalize) {
      for (col in colnames(sens_df)) {
        if (col != "participant") {
          col_max <- max(sens_df[[col]], na.rm = TRUE)
          if (!is.na(col_max) && col_max > 0) {
            sens_df[[col]] <- sens_df[[col]] / col_max
          }
        }
      }
    }
    
    # Convert to long format safely
    if (requireNamespace("tidyr", quietly = TRUE)) {
      sens_data <- tidyr::pivot_longer(sens_df, 
                                     cols = -participant, 
                                     names_to = "parameter", 
                                     values_to = "sensitivity")
    } else {
      # Fallback to reshape2
      sens_data <- reshape2::melt(sens_df, id.vars = "participant")
      colnames(sens_data) <- c("participant", "parameter", "sensitivity")
    }
    
    # Remove NAs
    sens_data <- sens_data[!is.na(sens_data$sensitivity), ]
    
    # Check if we have any data left
    if (nrow(sens_data) == 0) {
      return(ggplot() + 
               annotate("text", x = 0.5, y = 0.5, 
                       label = "No valid sensitivity data after removing NAs") +
               theme_void())
    }
    
    # Create plot
    ggplot(sens_data, aes(x = parameter, y = as.factor(participant), fill = sensitivity)) +
      geom_tile() +
      scale_fill_viridis_c(name = "Sensitivity", na.value = "white") +
      theme_minimal() +
      labs(title = "Parameter Sensitivity Across Participants",
           subtitle = ifelse(normalize, "Normalized by parameter", "Raw sensitivity values"),
           x = "Parameter",
           y = "Participant") +
      theme(axis.text.y = element_blank(),
            axis.ticks.y = element_blank())
  }, error = function(e) {
    # If processing fails, return a message
    message("Error in plot_sensitivity_heatmap: ", e$message)
    return(ggplot() + 
             annotate("text", x = 0.5, y = 0.5, 
                     label = paste("Error processing sensitivity data:", e$message)) +
             theme_void())
  })
}

#######################################
#' Plot average NLL profiles across participants
#' 
#' @param participant_results List of participant sensitivity results
#' @return List of ggplot objects with average NLL profiles
plot_average_nll_profiles <- function(participant_results) {
  # Print diagnostic info
  cat("Plotting average NLL profiles\n")
  cat("Number of participant results:", length(participant_results), "\n")

  # Extract all slice results with better diagnostics
  slice_data <- list()
  null_count <- 0
  error_count <- 0
  no_slices_count <- 0
  no_combined_count <- 0

  for (i in seq_along(participant_results)) {
    x <- participant_results[[i]]

    if (is.null(x)) {
      null_count <- null_count + 1
      next
    }

    if (isTRUE(x$error)) {
      error_count <- error_count + 1
      next
    }

    if (is.null(x$slices)) {
      no_slices_count <- no_slices_count + 1
      next
    }

    if (is.null(x$slices$combined_data)) {
      no_combined_count <- no_combined_count + 1
      next
    }

    # Only if we get here do we have valid slice data
    slice_data[[length(slice_data) + 1]] <- x$slices$combined_data
  }

  # Print diagnostics
  cat("Diagnostic counts:\n")
  cat("  Null results:", null_count, "\n")
  cat("  Error results:", error_count, "\n")
  cat("  No slices:", no_slices_count, "\n")
  cat("  No combined data:", no_combined_count, "\n")
  cat("  Valid slices:", length(slice_data), "\n")

  # Check if we have any valid slice data
  if (length(slice_data) == 0) {
    cat("No valid slice data found\n")
    # Return an empty plot
    return(list(empty_plot = ggplot() +
                 annotate("text", x = 0.5, y = 0.5,
                         label = "No valid NLL profile data available") +
                 theme_void()))
  }

  # Try to combine data without using rbind
  all_df <- data.frame(
    parameter = character(),
    value = numeric(),
    nll = numeric(),
    delta_nll = numeric(),
    value_norm = numeric()
  )

  # Manually combine data frames
  for (df in slice_data) {
    if (is.data.frame(df) && nrow(df) > 0) {
      # Calculate normalized value
      if (!("value_norm" %in% names(df))) {
        for (param in unique(df$parameter)) {
          param_data <- df[df$parameter == param, ]
          param_min <- min(param_data$value, na.rm = TRUE)
          param_max <- max(param_data$value, na.rm = TRUE)

          if (param_max > param_min) {
            df$value_norm[df$parameter == param] <-
              (param_data$value - param_min) / (param_max - param_min)
          } else {
            df$value_norm[df$parameter == param] <- 0.5
          }
        }
      }

      # Skip if normalized values are invalid
      if (any(is.na(df$value_norm))) {
        next
      }

      # Append to main data frame
      all_df <- rbind(all_df, df)
    }
  }

  # Check if we have a valid data frame
  if (nrow(all_df) == 0) {
    cat("No valid data after combining slices\n")
    return(list(empty_plot = ggplot() +
                 annotate("text", x = 0.5, y = 0.5,
                         label = "No valid data after combining profiles") +
                 theme_void()))
  }

  # Calculate average profiles manually instead of using dplyr
  param_values <- unique(all_df[, c("parameter", "value_norm")])
  avg_profiles <- data.frame()

  for (i in 1:nrow(param_values)) {
    param <- param_values$parameter[i]
    val_norm <- param_values$value_norm[i]

    subset <- all_df[all_df$parameter == param &
                    all_df$value_norm == val_norm, ]

    if (nrow(subset) > 0) {
      avg_profiles <- rbind(avg_profiles, data.frame(
        parameter = param,
        value_norm = val_norm,
        mean_delta_nll = mean(subset$delta_nll, na.rm = TRUE),
        sd_delta_nll = sd(subset$delta_nll, na.rm = TRUE),
        se_delta_nll = sd(subset$delta_nll, na.rm = TRUE) / sqrt(nrow(subset)),
        n = nrow(subset)
      ))
    }
  }

  # Check if we have valid average profiles
  if (nrow(avg_profiles) == 0) {
    cat("No valid average profiles\n")
    return(list(empty_plot = ggplot() +
                 annotate("text", x = 0.5, y = 0.5,
                         label = "No valid average profiles") +
                 theme_void()))
  }

  # # Create plots for each parameter
  # plots <- list()
  # params <- unique(avg_profiles$parameter)
  # 
  # for (param in params) {
  #   param_data <- avg_profiles[avg_profiles$parameter == param, ]
  # 
  #   if (nrow(param_data) > 0) {
  #     plots[[param]] <- ggplot(param_data, aes(x = value_norm, y = mean_delta_nll)) +
  #       geom_line(size = 1) +
  #       geom_ribbon(aes(ymin = mean_delta_nll - se_delta_nll,
  #                      ymax = mean_delta_nll + se_delta_nll),
  #                  alpha = 0.2) +
  #       theme_minimal() +
  #       labs(title = paste("Average NLL Profile for", param),
  #            subtitle = paste("Based on", max(param_data$n), "participants"),
  #            x = paste(param, "(Normalized Value)"),
  #            y = "Mean ∆NLL")
  #   } else {
  #     plots[[param]] <- ggplot() +
  #       annotate("text", x = 0.5, y = 0.5,
  #               label = paste("No data for parameter:", param)) +
  #       theme_void()
  #   }
  # }
  # 
  # return(plots)
  # Create a combined plot for all parameters
combined_plot <- ggplot(avg_profiles, aes(x = value_norm, y = mean_delta_nll, color = parameter, fill = parameter)) +
  geom_line(size = 1) +
  geom_ribbon(aes(ymin = mean_delta_nll - se_delta_nll, ymax = mean_delta_nll + se_delta_nll),
              alpha = 0.2, color = NA) +
  theme_minimal() +
  labs(title = "Average NLL Profiles Across Parameters",
       subtitle = "Each line shows the mean ΔNLL as the parameter deviates from its optimal value",
       x = "Normalized Parameter Value",
       y = "Mean ΔNLL") +
  theme(legend.title = element_blank())

# Return the combined plot
combined_plot

}


```

```{r}
plot_grid_results <- function(sensitivity_results) {
  # Extract grid results
  grid_results <- lapply(sensitivity_results$analysis_results$participant_results, 
                         function(x) {
                           if(!is.null(x$grid) && !is.null(x$grid$results_df)) {
                             return(x$grid$results_df)
                           }
                           return(NULL)
                         })
  
  # Filter out NULL results
  grid_results <- grid_results[!sapply(grid_results, is.null)]
  
  if(length(grid_results) == 0) {
    return(ggplot() + 
             annotate("text", x = 0.5, y = 0.5, 
                     label = "No grid search results available") +
             theme_void())
  }
  
  # Combine grid results
  combined_grid <- do.call(rbind, grid_results)
  
  # Create parameter pair combinations
  param_names <- names(combined_grid)[!names(combined_grid) %in% c("nll")]
  param_pairs <- combn(param_names, 2, simplify = FALSE)
  
  # Create plots for each parameter pair
  plots <- list()
  for(pair in param_pairs) {
    p1 <- pair[1]
    p2 <- pair[2]
    
    # Aggregate NLL values across the grid
    agg_data <- aggregate(nll ~ get(p1) + get(p2), data = combined_grid, FUN = mean)
    names(agg_data) <- c(p1, p2, "mean_nll")
    
    # Create heatmap
    plots[[paste(p1, "vs", p2)]] <- ggplot(agg_data, aes(x = .data[[p1]], y = .data[[p2]], fill = mean_nll)) +
      geom_tile() +
      scale_fill_viridis_c(name = "Mean NLL", direction = -1) +
      theme_minimal() +
      labs(title = paste("Grid Search Results:", p1, "vs", p2),
           x = p1, y = p2)
  }
  
  return(plots)
}

```


```{r}
#' Diagnose sensitivity analysis results
#' 
#' @param results Results from analyze_population_sensitivity
#' @return NULL (prints diagnostic information)
diagnose_sensitivity_results <- function(results) {
  cat("Top-level components in results:\n")
  print(names(results))
  
  cat("\nSummary components:\n")
  print(names(results$summary))
  
  cat("\nNumber of valid participants:", results$summary$n_participants, "\n")
  
  # Check sensitivity matrix
  if (!is.null(results$summary$sensitivity_matrix)) {
    cat("\nSensitivity matrix information:\n")
    cat("Dimensions:", dim(results$summary$sensitivity_matrix), "\n")
    
    if (is.matrix(results$summary$sensitivity_matrix) && ncol(results$summary$sensitivity_matrix) > 0) {
      cat("Column names:", paste(colnames(results$summary$sensitivity_matrix), collapse=", "), "\n")
      
      # Check if matrix has any valid values
      cat("Number of non-NA values:", sum(!is.na(results$summary$sensitivity_matrix)), "\n")
      
      # If matrix is empty or all NA, this could be a problem
      if (sum(!is.na(results$summary$sensitivity_matrix)) == 0) {
        cat("WARNING: Sensitivity matrix contains no valid values\n")
      }
    } else {
      cat("WARNING: Sensitivity matrix has 0 columns or is not a matrix\n")
    }
  } else {
    cat("WARNING: Sensitivity matrix is NULL\n")
  }
  
  # Check participant results
  if (!is.null(results$participant_results)) {
    cat("\nParticipant results:\n")
    cat("Number of participants:", length(results$participant_results), "\n")
    
    # Check how many succeeded vs. failed
    n_valid <- sum(!sapply(results$participant_results, is.null))
    n_error <- sum(sapply(results$participant_results, function(x) isTRUE(x$error)))
    
    cat("Number of valid results:", n_valid, "\n")
    cat("Number with errors:", n_error, "\n")
    
    # Print a sample of errors
    if (n_error > 0) {
      cat("\nSample of errors:\n")
      error_msgs <- sapply(results$participant_results, function(x) {
        if (isTRUE(x$error)) return(x$message) else return(NULL)
      })
      error_msgs <- error_msgs[!sapply(error_msgs, is.null)]
      
      # Print first few unique errors
      unique_errors <- unique(error_msgs)
      for (i in 1:min(3, length(unique_errors))) {
        cat(i, ":", unique_errors[i], "\n")
      }
    }
    
    # Check a sample of valid results
    if (n_valid > 0) {
      cat("\nChecking first valid result:\n")
      valid_idx <- which(!sapply(results$participant_results, is.null) & 
                        !sapply(results$participant_results, function(x) isTRUE(x$error)))[1]
      
      if (!is.na(valid_idx)) {
        valid_result <- results$participant_results[[valid_idx]]
        cat("Has FIM:", !is.null(valid_result$fim), "\n")
        cat("Has slices:", !is.null(valid_result$slices), "\n")
        
        if (!is.null(valid_result$slices) && !is.null(valid_result$slices$sensitivity)) {
          cat("Sensitivities:", names(valid_result$slices$sensitivity), "\n")
        } else {
          cat("WARNING: No slice sensitivity in valid result\n")
        }
      }
    }
  }
  
  invisible(NULL)
}
```

## Visualise fim results 

function to plot results of the Fisher Information Matrix analysis 

These visualizations complement each other, providing multiple perspectives on parameter sensitivity from distribution summaries to detailed interaction patterns.

### Condition Number

The condition number is a measure of how "well-behaved" the parameter estimation problem is. It's calculated as the ratio of the largest eigenvalue to the smallest eigenvalue of the Fisher Information Matrix.

<!-- Low condition number (close to 1):  stable and easy -->
<!-- High condition number (>100):unstable and sensitive -->

<!-- Mathematical insight: If you have a high condition number, small errors in your data can lead to large changes in parameter estimates. Specifically, the relative error in your parameter estimates could be up to [condition number] times the relative error in your data. -->


### Eigenvalue Spectrum

The eigenvalue spectrum refers to the set of all eigenvalues of the Fisher Information Matrix, typically arranged in descending order. 

<!-- Eigenvalues represent the amount of information available in particular directions in parameter space. Each eigenvalue corresponds to an eigenvector, which represents a specific combination of parameters. -->

<!-- A sharp drop-off in the eigenvalue spectrum: Indicates that only a few parameter combinations are well-determined by the data. -->
<!-- Gradual decline: Suggests that information is more evenly distributed across parameters. -->
<!-- Very small eigenvalues: Correspond to parameter combinations that are barely constrained by the data. -->

```{r, fim_plots}
#' Visualize Fisher Information Matrix diagonal elements (parameter precision)
#' with improved diagnostics and visualization
#' @param sensitivity_results Results from run_sensitivity_analysis
#' @return ggplot object with precision boxplots and diagnostic information
plot_fim_precision <- function(sensitivity_results) {
  # Extract participant-level FIM results with better diagnostics
  fim_results <- list()
  valid_count <- 0
  total_count <- 0
  
  # Collect all available FIM results with diagnostics
  for (p_id in names(sensitivity_results$analysis_results$participant_results)) {
    x <- sensitivity_results$analysis_results$participant_results[[p_id]]
    total_count <- total_count + 1
    
    if (!is.null(x) && !is.null(x$fim) && !is.null(x$fim$parameter_precision)) {
      valid_count <- valid_count + 1
      
      precision_values <- x$fim$parameter_precision
      
      # Check for abnormal values
      if (any(!is.finite(precision_values)) || any(precision_values < 0)) {
        cat("Skipping participant", p_id, "with invalid precision values:", 
            toString(precision_values), "\n")
        next
      }
      
      # Create data frame
      precision_df <- data.frame(
        participant = p_id,
        parameter = names(precision_values),
        precision = as.numeric(precision_values)
      )
      
      fim_results[[length(fim_results) + 1]] <- precision_df
    }
  }
  
  # Combine results and check for emptiness
  if (length(fim_results) == 0) {
    cat("No valid precision values found out of", total_count, "participants\n")
    return(ggplot() + 
             annotate("text", x = 0.5, y = 0.5, 
                      label = "No valid FIM precision data available") +
             theme_void())
  }
  
  fim_precision_df <- do.call(rbind, fim_results)
  
  # Print diagnostics
  cat("FIM Precision Diagnostics:\n")
  cat("- Total participants:", total_count, "\n")
  cat("- Participants with valid FIM:", valid_count, 
      "(", round(valid_count/total_count*100, 1), "%)\n")
  
  # Count by parameter
  param_counts <- table(fim_precision_df$parameter)
  cat("- Valid precision counts by parameter:\n")
  for (param in names(param_counts)) {
    cat("  ", param, ":", param_counts[param], "\n")
  }
  
  # Basic stats on precision values
  precision_stats <- tapply(fim_precision_df$precision, fim_precision_df$parameter, 
                           function(x) c(min=min(x), median=median(x), 
                                         max=max(x), mean=mean(x)))
  cat("- Precision value ranges:\n")
  for (param in names(precision_stats)) {
    stats <- precision_stats[[param]]
    cat("  ", param, ": min =", format(stats["min"], scientific=TRUE), 
        ", median =", format(stats["median"], scientific=TRUE),
        ", max =", format(stats["max"], scientific=TRUE), "\n")
  }
  
  # Check for extreme ranges that might cause visualization issues
  range_ratio <- max(fim_precision_df$precision) / min(fim_precision_df$precision)
  cat("- Precision value range ratio (max/min):", format(range_ratio, scientific=TRUE), "\n")
  
  # Filter extreme outliers if needed for visualization
  if (range_ratio > 1e12) {
    cat("Warning: Extreme range detected, filtering for visualization\n")
    
    # Calculate more robust filtering thresholds by parameter
    for (param in unique(fim_precision_df$parameter)) {
      param_vals <- fim_precision_df$precision[fim_precision_df$parameter == param]
      if (length(param_vals) > 5) {
        q1 <- quantile(param_vals, 0.05)
        q3 <- quantile(param_vals, 0.95)
        iqr <- q3 - q1
        lower_bound <- max(q1 - 3 * iqr, min(param_vals) / 10)
        upper_bound <- min(q3 + 3 * iqr, max(param_vals) * 10)
        
        # Apply filter
        filtered_idx <- which(fim_precision_df$parameter == param & 
                             (fim_precision_df$precision < lower_bound | 
                              fim_precision_df$precision > upper_bound))
        
        if (length(filtered_idx) > 0) {
          cat("  Filtered", length(filtered_idx), "outliers from", param, 
              "for visualization (keeping", length(param_vals) - length(filtered_idx), "values)\n")
          fim_precision_df <- fim_precision_df[-filtered_idx, ]
        }
      }
    }
  }
  
  # Check if we still have data after filtering
  if (nrow(fim_precision_df) == 0) {
    return(ggplot() + 
             annotate("text", x = 0.5, y = 0.5, 
                      label = "No valid data after filtering extreme values") +
             theme_void())
  }
  
  # Create boxplot with more resilient log scale
  p <- ggplot(fim_precision_df, aes(x = parameter, y = precision)) +
    geom_boxplot(fill = "lightgreen", alpha = 0.6, outlier.shape = NA) + # Hide outliers in boxplot
    geom_jitter(width = 0.2, alpha = 0.5, color = "darkgreen", size = 1) +
    scale_y_log10(
      labels = scales::scientific_format(digits = 2),
      breaks = scales::trans_breaks("log10", function(x) 10^x, n = 5)
    ) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          axis.text.y = element_text(size = 10)) +
    labs(title = "Parameter Precision from Fisher Information Matrix",
         subtitle = paste("Based on", valid_count, "of", total_count, 
                          "participants (", round(valid_count/total_count*100, 1), "%)"),
         x = "Parameter",
         y = "Precision (Log Scale)")
  
  return(p)
}


#' Visualize FIM correlation structure
#' 
#' @param sensitivity_results Results from run_sensitivity_analysis
#' @return ggplot object with correlation heatmap
plot_fim_correlations <- function(sensitivity_results) {
  # Extract standardized FIM matrices
  fim_std_matrices <- lapply(sensitivity_results$analysis_results$participant_results, function(x) {
    if (!is.null(x) && !is.null(x$fim) && !is.null(x$fim$fim_standardized)) {
      # Return both the matrix and its dimension info
      list(
        matrix = x$fim$fim_standardized,
        param_names = colnames(x$fim$fim_standardized),
        dims = dim(x$fim$fim_standardized)
      )
    } else {
      NULL
    }
  })
  
  # Remove NULL entries
  fim_std_matrices <- fim_std_matrices[!sapply(fim_std_matrices, is.null)]
  
  # Check if we have any matrices
  if (length(fim_std_matrices) == 0) {
    return(ggplot() + 
             annotate("text", x = 0.5, y = 0.5, 
                     label = "No valid FIM correlation data available") +
             theme_void())
  }
  
  # Find the most common parameter set
  param_sets <- lapply(fim_std_matrices, function(x) paste(sort(x$param_names), collapse=","))
  param_set_table <- table(unlist(param_sets))
  most_common_set <- names(param_set_table)[which.max(param_set_table)]
  common_params <- sort(strsplit(most_common_set, ",")[[1]])
  n_params <- length(common_params)
  
  # Filter matrices with the same parameter set
  valid_matrices <- list()
  for (i in seq_along(fim_std_matrices)) {
    mat_info <- fim_std_matrices[[i]]
    if (length(mat_info$param_names) == n_params && 
        all(sort(mat_info$param_names) == common_params)) {
      # Ensure consistent parameter order
      ordered_mat <- matrix(0, nrow=n_params, ncol=n_params)
      rownames(ordered_mat) <- common_params
      colnames(ordered_mat) <- common_params
      
      for (r in 1:n_params) {
        for (c in 1:n_params) {
          r_idx <- which(mat_info$param_names == common_params[r])
          c_idx <- which(mat_info$param_names == common_params[c])
          if (length(r_idx) > 0 && length(c_idx) > 0) {
            ordered_mat[r, c] <- mat_info$matrix[r_idx, c_idx]
          }
        }
      }
      valid_matrices[[length(valid_matrices) + 1]] <- ordered_mat
    }
  }
  
  # Check if we have enough valid matrices
  if (length(valid_matrices) < 1) {
    return(ggplot() + 
             annotate("text", x = 0.5, y = 0.5, 
                     label = "No consistent FIM matrices found") +
             theme_void())
  }
  
  # Calculate average correlation matrix
  avg_corr_matrix <- matrix(0, nrow = n_params, ncol = n_params)
  rownames(avg_corr_matrix) <- common_params
  colnames(avg_corr_matrix) <- common_params
  
  # Sum up all matrices
  count_matrix <- matrix(0, nrow = n_params, ncol = n_params)
  for (mat in valid_matrices) {
    # Replace NAs with zeros for summing
    mat_clean <- mat
    mat_clean[is.na(mat_clean)] <- 0
    avg_corr_matrix <- avg_corr_matrix + mat_clean
    
    # Track how many non-NA values we have for each cell
    count_matrix <- count_matrix + (!is.na(mat))
  }
  
  # Divide by count to get average (avoid division by zero)
  count_matrix[count_matrix == 0] <- 1  # Avoid division by zero
  avg_corr_matrix <- avg_corr_matrix / count_matrix
  
  # Convert to long format for ggplot
  corr_data <- reshape2::melt(avg_corr_matrix)
  names(corr_data) <- c("Parameter1", "Parameter2", "Correlation")
  
  # Create heatmap
  ggplot(corr_data, aes(x = Parameter1, y = Parameter2, fill = Correlation)) +
    geom_tile() +
    scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                         midpoint = 0, limit = c(-1,1), space = "Lab",
                         name="Correlation") +
    theme_minimal() +
    labs(title = "Average Parameter Correlations from FIM",
         subtitle = paste("Based on", length(valid_matrices), "consistent participants")) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
}

#' Analyze eigenvalues and condition numbers from FIM
#' 
#' @param sensitivity_results Results from run_sensitivity_analysis
#' @return List with plots and summary statistics
analyze_fim_eigenvalues <- function(sensitivity_results) {
  # Extract eigenvalues and condition numbers
  eig_results <- lapply(sensitivity_results$analysis_results$participant_results, function(x) {
    if (!is.null(x) && !is.null(x$fim) && !is.null(x$fim$eigenvalues)) {
      list(
        participant = ifelse(!is.null(x$participant_data_summary$playerId), 
                            x$participant_data_summary$playerId, NA),
        eigenvalues = x$fim$eigenvalues,
        condition_number = x$fim$condition_number
      )
    } else {
      NULL
    }
  })
  
  # Remove NULL entries
  eig_results <- eig_results[!sapply(eig_results, is.null)]
  
  # Check if we have any results
  if (length(eig_results) == 0) {
    return(list(
      message = "No valid eigenvalue data available",
      condition_plot = ggplot() + theme_void() + 
        annotate("text", x = 0.5, y = 0.5, label = "No valid condition number data"),
      eigenvalue_plot = ggplot() + theme_void() + 
        annotate("text", x = 0.5, y = 0.5, label = "No valid eigenvalue data")
    ))
  }
  
  # Extract condition numbers
  condition_numbers <- sapply(eig_results, function(x) x$condition_number)
  condition_numbers <- condition_numbers[!is.na(condition_numbers) & 
                                        is.finite(condition_numbers) & 
                                        condition_numbers > 0 & 
                                        condition_numbers < 1e6]  # Filter extreme values
  
  # Skip condition number plot if no valid data
  if (length(condition_numbers) == 0) {
    condition_plot <- ggplot() + theme_void() + 
      annotate("text", x = 0.5, y = 0.5, label = "No valid condition number data")
  } else {
    # Create condition number histogram
    condition_plot <- ggplot(data.frame(condition = condition_numbers), aes(x = condition)) +
      geom_histogram(fill = "coral", color = "black", alpha = 0.7, bins = min(30, length(condition_numbers)/3)) +
      scale_x_log10() +
      theme_minimal() +
      labs(title = "Distribution of FIM Condition Numbers",
           subtitle = paste("Based on", length(condition_numbers), "participants"),
           x = "Condition Number (Log Scale)",
           y = "Count") +
      geom_vline(xintercept = 100, linetype = "dashed", color = "red") +
      annotate("text", x = 120, y = max(table(cut(condition_numbers, 
                                                min(30, length(condition_numbers)/3))))/2, 
               label = "Threshold = 100", color = "red", hjust = 0)
  }
  
  # Find most common number of eigenvalues
  n_eigenvalues <- sapply(eig_results, function(x) length(x$eigenvalues))
  n_eigenvalues <- n_eigenvalues[n_eigenvalues > 0]
  
  if (length(n_eigenvalues) == 0) {
    eigenvalue_plot <- ggplot() + theme_void() + 
      annotate("text", x = 0.5, y = 0.5, label = "No valid eigenvalue data")
    
    return(list(
      condition_plot = condition_plot,
      eigenvalue_plot = eigenvalue_plot,
      summary_stats = list(
        mean_condition_number = ifelse(length(condition_numbers) > 0, 
                                     mean(condition_numbers, na.rm = TRUE), NA),
        median_condition_number = ifelse(length(condition_numbers) > 0, 
                                       median(condition_numbers, na.rm = TRUE), NA),
        percent_above_threshold = ifelse(length(condition_numbers) > 0, 
                                       mean(condition_numbers > 100, na.rm = TRUE) * 100, NA)
      ),
      condition_numbers = condition_numbers
    ))
  }
  
  # Find most common eigenvalue count
  most_common_count <- as.numeric(names(which.max(table(n_eigenvalues))))
  
  # Extract eigenvalue data from matrices with this count
  eigenvalue_data <- data.frame()
  for (i in seq_along(eig_results)) {
    evals <- eig_results[[i]]$eigenvalues
    if (length(evals) == most_common_count && all(!is.na(evals)) && all(is.finite(evals))) {
      # Filter negative or zero eigenvalues (shouldn't happen in theory but might in practice)
      evals <- pmax(evals, 1e-10)
      
      tmp_df <- data.frame(
        participant = rep(i, length(evals)),
        eigenvalue_index = 1:length(evals),
        value = evals
      )
      eigenvalue_data <- rbind(eigenvalue_data, tmp_df)
    }
  }
  
  # Create eigenvalue scree plot
  if (nrow(eigenvalue_data) > 0) {
    # Summarize eigenvalues across participants
    eigen_summary <- aggregate(value ~ eigenvalue_index, data = eigenvalue_data, 
                              FUN = function(x) c(mean = mean(x, na.rm = TRUE), 
                                                 sd = sd(x, na.rm = TRUE)))
    eigen_summary_df <- data.frame(
      eigenvalue_index = eigen_summary$eigenvalue_index,
      mean_value = sapply(eigen_summary$value, function(x) x[1]),
      sd_value = sapply(eigen_summary$value, function(x) x[2])
    )
    
    eigenvalue_plot <- ggplot(eigen_summary_df, aes(x = eigenvalue_index, y = mean_value)) +
      geom_line(size = 1) +
      geom_point(size = 3, color = "blue") +
      geom_errorbar(aes(ymin = pmax(mean_value - sd_value, 1e-10), 
                        ymax = mean_value + sd_value), width = 0.2) +
      scale_y_log10() +
      theme_minimal() +
      labs(title = "Eigenvalue Spectrum of Fisher Information Matrix",
           subtitle = paste("Average across", length(unique(eigenvalue_data$participant)), 
                           "participants with consistent eigenvalue count"),
           x = "Eigenvalue Index",
           y = "Eigenvalue (Log Scale)")
  } else {
    eigenvalue_plot <- ggplot() + theme_void() + 
      annotate("text", x = 0.5, y = 0.5, label = "No consistent eigenvalue data")
  }
  
  # Calculate summary statistics
  summary_stats <- list(
    mean_condition_number = ifelse(length(condition_numbers) > 0, 
                                 mean(condition_numbers, na.rm = TRUE), NA),
    median_condition_number = ifelse(length(condition_numbers) > 0, 
                                   median(condition_numbers, na.rm = TRUE), NA),
    percent_above_threshold = ifelse(length(condition_numbers) > 0, 
                                   mean(condition_numbers > 100, na.rm = TRUE) * 100, NA)
  )
  
  # Return results
  return(list(
    condition_plot = condition_plot,
    eigenvalue_plot = eigenvalue_plot,
    summary_stats = summary_stats,
    condition_numbers = condition_numbers
  ))
}

# Function to run all FIM visualizations
visualize_fim_results <- function(sensitivity_results) {
  precision_plot <- plot_fim_precision(sensitivity_results)
  correlation_plot <- plot_fim_correlations(sensitivity_results)
  eigenvalue_results <- analyze_fim_eigenvalues(sensitivity_results)
  
  # Print summary statistics
  cat("=== FIM Analysis Summary ===\n")
  cat("Mean condition number:", round(eigenvalue_results$summary_stats$mean_condition_number, 2), "\n")
  cat("Median condition number:", round(eigenvalue_results$summary_stats$median_condition_number, 2), "\n")
  cat("Percent with condition number > 100:", 
      round(eigenvalue_results$summary_stats$percent_above_threshold, 1), "%\n")
  
  # Return all plots
  return(list(
    precision_plot = precision_plot,
    correlation_plot = correlation_plot,
    condition_plot = eigenvalue_results$condition_plot,
    eigenvalue_plot = eigenvalue_results$eigenvalue_plot
  ))
}


```

```{r}
#' Analyze FIM calculation success rates and parameter precision statistics
#' 
#' @param sensitivity_results Results from run_sensitivity_analysis
#' @return List with FIM calculation success metrics and precision statistics
analyze_fim_success <- function(sensitivity_results) {
  total_participants <- length(sensitivity_results$analysis_results$participant_results)
  
  # Initialize counters and containers
  valid_fim_count <- 0
  valid_precision_count <- 0
  valid_eigenvalues_count <- 0
  valid_condition_count <- 0
  
  precision_values <- list()
  condition_numbers <- numeric()
  
  # Check each participant
  for (p_id in names(sensitivity_results$analysis_results$participant_results)) {
    x <- sensitivity_results$analysis_results$participant_results[[p_id]]
    
    if (!is.null(x) && !is.null(x$fim)) {
      valid_fim_count <- valid_fim_count + 1
      
      # Check precision values
      if (!is.null(x$fim$parameter_precision)) {
        precision <- x$fim$parameter_precision
        
        if (all(is.finite(precision))) {
          valid_precision_count <- valid_precision_count + 1
          
          # Store by parameter
          for (param_name in names(precision)) {
            if (!param_name %in% names(precision_values)) {
              precision_values[[param_name]] <- numeric()
            }
            precision_values[[param_name]] <- c(precision_values[[param_name]], 
                                               precision[param_name])
          }
        }
      }
      
      # Check eigenvalues
      if (!is.null(x$fim$eigenvalues)) {
        if (all(is.finite(x$fim$eigenvalues))) {
          valid_eigenvalues_count <- valid_eigenvalues_count + 1
        }
      }
      
      # Check condition number
      if (!is.null(x$fim$condition_number)) {
        if (is.finite(x$fim$condition_number)) {
          valid_condition_count <- valid_condition_count + 1
          condition_numbers <- c(condition_numbers, x$fim$condition_number)
        }
      }
    }
  }
  
  # Compute statistics for precision values by parameter
  precision_stats <- list()
  for (param in names(precision_values)) {
    if (length(precision_values[[param]]) > 0) {
      vals <- precision_values[[param]]
      precision_stats[[param]] <- list(
        count = length(vals),
        min = min(vals),
        max = max(vals),
        median = median(vals),
        mean = mean(vals),
        percentiles = quantile(vals, c(0.05, 0.25, 0.75, 0.95))
      )
    }
  }
  
  # Compute condition number statistics
  condition_stats <- if (length(condition_numbers) > 0) {
    list(
      count = length(condition_numbers),
      min = min(condition_numbers),
      max = max(condition_numbers),
      median = median(condition_numbers),
      mean = mean(condition_numbers),
      above_threshold = sum(condition_numbers > 100),
      pct_above_threshold = sum(condition_numbers > 100) / length(condition_numbers) * 100
    )
  } else {
    list(count = 0)
  }
  
  # Return all analysis results
  return(list(
    total_participants = total_participants,
    valid_fim_count = valid_fim_count,
    valid_fim_percent = valid_fim_count / total_participants * 100,
    valid_precision_count = valid_precision_count,
    valid_precision_percent = valid_precision_count / total_participants * 100,
    valid_eigenvalues_count = valid_eigenvalues_count,
    valid_eigenvalues_percent = valid_eigenvalues_count / total_participants * 100,
    valid_condition_count = valid_condition_count,
    valid_condition_percent = valid_condition_count / total_participants * 100,
    precision_stats = precision_stats,
    condition_stats = condition_stats
  ))
}


```


# 10. Complete Analysis Function


The function `run_sensitivity_analysis` orchestrates the entire sensitivity analysis pipeline, providing a single entry point for the complete workflow. It coordinates:

1. Setting up the analysis environment, including timing the process
2. Defining parameter grids and ranges for comprehensive exploration
3. Running the population-level sensitivity analysis
4. Generating all visualizations from the results
5. Packaging everything into a structured output

The function includes sensible defaults for parameter ranges based on theoretical constraints and common values from literature:
- alpha (learning rate): 0.01 to 0.99
- temp (temperature): 0.1 to 10
- envy: 0 to 6
- guilt: 0 to 5

<!-- It supports parallel processing through the n_cores parameter, allowing users to leverage multi-core systems for faster analysis. The optional grid-based analysis can be toggled on/off depending on computational resources and time constraints. -->

<!-- The comprehensive results structure includes both the raw analysis results and the generated visualizations, enabling both programmatic access to the sensitivity metrics and immediate visual interpretation. The function also tracks and reports the total run time, providing useful information for planning future analyses. -->


```{r complete_analysis}
#' Run complete sensitivity analysis for MFRL model
#' 
#' @param data Participant data (dataframe with playerId column or list)
#' @param fitted_params Dataframe with fitted parameters (must have playerId column)
#' @param model_fn Function that calculates negative log-likelihood
#' @param n_cores Number of cores for parallel processing
#' @param include_grid Whether to include grid-based sensitivity analysis
#' @return List with complete sensitivity analysis results
run_sensitivity_analysis <- function(data, fitted_params, model_fn, 
                                   n_cores = 1, include_grid = FALSE,use_global_ranges = TRUE) {
  # Start timing
  start_time <- Sys.time()
  
  # Set up parameter grid if requested
  param_grid <- NULL
  if (include_grid) {
    param_grid <- list(
      alpha = seq(0.05, 0.99, length.out = 10),
      temp = seq(0.2, 10, length.out = 10),
      envy = seq(0.1, 6, length.out = 10),
      guilt = seq(0.1, 5, length.out = 10)
    )
  }
  
  global_param_ranges <- list(
  alpha = c(0.01, 0.99),
  temp = c(0.1, 15),
  envy = c(0, 6),
  guilt = c(0, 5)
)
  
  # Run analysis
  results <- analyze_population_sensitivity(
    model_fn = model_fn,
    data_list = data,
    params_df = fitted_params,
    param_grid = param_grid,
    global_param_ranges = global_param_ranges, 
    n_cores = n_cores
  )
  
  # Generate plots
  sensitivity_boxplots <- plot_sensitivity_boxplots(results$summary)
  sensitivity_heatmap <- plot_sensitivity_heatmap(results$summary)
  nll_profiles <- plot_average_nll_profiles(results$participant_results)
  
  # End timing
  end_time <- Sys.time()
  run_time <- difftime(end_time, start_time, units = "mins")
  
  # Return complete results
  return(list(
    analysis_results = results,
    plots = list(
      sensitivity_boxplots = sensitivity_boxplots,
      sensitivity_heatmap = sensitivity_heatmap,
      nll_profiles = nll_profiles
    ),
    run_time = run_time
  ))
}
```

# 11. Results 

The results section presents the findings from applying our sensitivity analysis framework to the full dataset of 183 participants. The function pipeline is as follows:

1. `run_sensitivity_analysis` takes the complete dataset with all participants
2. `analyze_population_sensitivity` automatically splits this data by participant ID
3. Each participant's data is analyzed individually (in parallel across our cores)
4. `summarize_population_sensitivity` aggregates results across all participants
5. The visualization functions show the distribution of sensitivity across our entire population

The analysis calculates a comprehensive set of sensitivity metrics and visualizations, including boxplots of parameter sensitivity, heatmaps showing sensitivity across participants, average NLL profiles, and grid search results revealing parameter interactions.

**Mean Parameter Sensitivity:**

* temp (temperature): 334.002
* guilt: 219.94
* alpha (learning rate): 71.508
* envy: 7.768

These values indicate sufficient sensitivity for all parameters, though envy's sensitivity is notably lower.


```{r example_usage, eval=TRUE, cache=TRUE, include=FALSE}
# Load our data and fitted parameters
fitted_params <- read.csv("fit_result_MFRL_all_37.csv")
participant_data <- read.csv("data/final_data.csv")  



# Run sensitivity analysis
sensitivity_results <- run_sensitivity_analysis(
  data = participant_data,
  fitted_params = fitted_params,
  model_fn = direct_learning_model,  # our MFRL model function
  n_cores = 9,                      # Adjust if needed based on number of cores
  include_grid = TRUE             # Set to TRUE for more comprehensive grid analysis
)


# Get parameter recommendations
param_sensitivities <- sensitivity_results$analysis_results$summary$slice_sensitivity_summary

# Print recommendations
cat("Parameter Sensitivity Mean:\n")
for (param in names(param_sensitivities)) {
  cat(param, ": Mean sensitivity =", 
      round(param_sensitivities[[param]]$mean, 3), "\n")
}

# cat("\nRecommendations:\n")
# for (param in names(param_sensitivities)) {
#   if (param_sensitivities[[param]]$mean < 1) {  # 1 here is a bit of an arbitrary threshold but works well since big diff 
#     cat(param, "has low sensitivity and could be fixed to literature values.\n")
#   } else {
#     cat(param, "has sufficient sensitivity and can be estimated from the data.\n")
#   }
# }
```



### Boxplot Visualization


```{r, include=TRUE}
print(sensitivity_results$plots$sensitivity_boxplots)
```

This boxplot displays the distribution of sensitivity values for each parameter across all 183 participants. 

The boxplot illustrates the distribution of parameter sensitivity across participants.

**Key Findings:**

* **Parameter Hierarchy:** temp > guilt > alpha > envy, indicating varying levels of influence on model fit.
* **Distribution:** Right-skewed distributions across all parameters, spanning multiple orders of magnitude.
* **Individual Variation:** Significant spread in sensitivity values, highlighting individual differences in parameter influence.
* **Implications:** temp and guilt are crucial for model fit; envy may be less critical.


### Heatmap Visualization

```{r, include=TRUE}
print(sensitivity_results$plots$sensitivity_heatmap)
```

The heatmap displays participant-level parameter sensitivity.

**Key Findings:**

* **Parameter Hierarchy:** Consistent with boxplot results, temp and guilt show higher sensitivity.
* **Individual Differences:** Heterogeneity in sensitivity across participants, suggesting varied decision-making strategies.
* **Parameter Robustness:** Higher sensitivity parameters (temp, guilt) offer more reliable estimates.
* **Modeling Implications:** All parameters are estimable, but envy's reliability is lower.


### Average NLL Profiles

```{r, include=TRUE}
print(sensitivity_results$plots$nll_profiles)
```


This plot illustrates how the model fit (measured by negative log-likelihood) changes as each parameter is varied. 

**Key Findings:**

* **temp:** High sensitivity at low values, crucial for accurate estimation.
* **guilt:** High sensitivity at high values, overestimation worsens fit.
* **alpha:** Moderate sensitivity, optimal values in the mid-range.
* **envy:** Minimal sensitivity, suggesting potential for fixing this parameter.
* **Practical Recommendations:** Careful estimation of temp and guilt; flexibility for alpha; potential exclusion of envy.




### FIM results 

The Fisher Information Matrix analysis revealed numerical instability, with only 3 out of 183 participants yielding valid precision values. Negative precision values and low condition numbers suggest potential boundary issues or flat likelihood surfaces, particularly for the envy parameter. Valid results confirmed the parameter hierarchy (temp > guilt > alpha > envy).


```{r fim_results, include=FALSE}

fim_results <- visualize_fim_results(sensitivity_results)
# print(fim_results$precision_plot)
# print(fim_results$correlation_plot)
# print(fim_results$condition_plot)

# Usage:
# fim_analysis <- analyze_fim_success(sensitivity_results)
# print(fim_analysis)
```
```{r eigenplot, include=TRUE}
print(fim_results$eigenvalue_plot)
```


### Grid Search Results


Grid search heatmaps visualize parameter interaction effects on model fit.

**Key Findings:**

* **temp:** High sensitivity at low values, optimal fit at higher values.
* **guilt:** High sensitivity at high values, optimal fit at lower values.
* **alpha:** Moderate sensitivity, optimal fit in the mid-range.
* **envy:** Minimal sensitivity, negligible impact on fit.
* **Parameter Interactions:** Alpha and temp show a trade-off; envy’s limited influence confirmed; guilt influences fit, particularly with temp.
* **Implications:** Reinforces the potential to fix envy.

```{r, include=TRUE}
# Plot grid sensitivity results
grid_plots <- plot_grid_results(sensitivity_results)

# arrange multiple plots in a grid
if(length(grid_plots) > 1) {
  gridExtra::grid.arrange(grobs = grid_plots[1:min(6, length(grid_plots))], ncol = 2)
}
```

### Overall Implications

The consistent findings across analyses highlight the importance of temp and guilt, while suggesting envy's reduced role. This information aids parameter estimation and model simplification.


# Appendix: 

## Parameter Normalization Process

This outlines how parameter normalization is performed in the sensitivity analysis.

**1. Sensitivity Analysis for Each Participant and Parameter:**

* For every participant and each parameter (e.g., envy, guilt, temperature), the analysis begins.
* It starts with the participant's best-fitting parameter value (e.g., envy = 1.5).
* A range of parameter values around this optimum is tested (e.g., envy = 0.5, 1.0, 1.5, 2.0, 2.5).
* For each tested value, the change in model fit (Negative Log-Likelihood, NLL) is calculated.

**2. Normalization per Participant:**

* The normalization is done independently for each participant.
* For each parameter, the minimum and maximum tested parameter values are identified.
* This range is then rescaled to a 0-1 scale, where:
    * 0 represents the minimum tested value for that participant and parameter.
    * 1 represents the maximum tested value for that participant and parameter.
* This normalization allows for averaging sensitivity curves across participants, even if they have different:
    * Optimal parameter values.
    * Testing ranges.

**3. Concrete Example (Envy Parameter):**

* **Participant 1:** Tested values 0.5-2.5 are normalized to 0-1.
* **Participant 2:** Tested values 1.0-3.0 are normalized to 0-1.
* **Participant 3:** Tested values 0.2-1.2 are normalized to 0-1.

**4. Interpretation of Results:**

* The resulting plot displays the average sensitivity of the model fit across participants along the normalized 0-1 range.
* A high ∆NLL (change in NLL) at x=0.2 indicates: "When the parameter is 20% of the way from its minimum to maximum tested value, the model fit is typically this much worse than at the optimum."